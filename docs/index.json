[{"content":"An important moment has happened recently - I\u0026rsquo;ve graduated with a First Class BSc degree in Computer Science from the University of Birmingham! To my surprise, during the graduation ceremony, I have also been awarded the Distinguished Dissertation Prize 2020/2021 for the highest-graded dissertation in the graduating cohort. This was quite an honour as it came completely unexpected, and it made a lot of the effort that went into writing my dissertation worthwhile.\nI was going to write a post reflecting on my dissertation at some point. Given that I\u0026rsquo;ve received such an accolade, however, I thought it made sense to document and pass on some knowledge that I believe went into making my dissertation stand out.\nDissertation Context The dissertation accounted for a third of the overall mark for my final year. I chose to base mine on cybersecurity, as that\u0026rsquo;s the topic that most aligns with my interests in terms of academics. I\u0026rsquo;ve been appointed a cybersecurity researcher as my dissertation supervisor. He was incredibly helpful and without a doubt, a lot of the success of my dissertation is thanks to his input. My dissertation was titled \u0026ldquo;Analysing the Security of 4G LTE Networks using an Independently Developed MITM Proxy/Relay\u0026rdquo;. This is quite a mouthful, but the specifics of it don\u0026rsquo;t matter for this blog post. Overall, I received a mark of 90% in my dissertation.\nI compiled a list of 10 tips that helped me work on my dissertation below. Some tips I\u0026rsquo;ve received from my supervisor, some I figured out as I worked on my dissertation, and some I found through discussion with my peers. I tried to keep them general, yet specific enough to the field of computer science. I hope they\u0026rsquo;re useful and provide something new compared to existing advice online!\nThe 10 Tips 1) Set many, achievable goals üèÅ Your dissertation will feel overwhelming at first. There is a lot of work to do over the year and it may feel like there\u0026rsquo;s no way you\u0026rsquo;d be able to finish it all. I overcame this by setting myself milestones that I discussed with my supervisor. I then split those milestones into smaller goals and sub-goals.\nAs for milestones, make sure you have some small ones so that it feels like you made progress early on. Equally, make sure to have some ambitious ones towards the end. It\u0026rsquo;s great to have an \u0026ldquo;impossible\u0026rdquo; goal to strive towards as then you\u0026rsquo;ll always feel like there is work to do, which can also help with eliminating procrastination. In the worst case, if those difficult goals aren\u0026rsquo;t achieved, you can mention them as potential improvements. In the best case, you\u0026rsquo;ll achieve those goals and have a substantial contribution to the field in your dissertation.\nIt\u0026rsquo;s also important to assign a priority to your goals. I split my tasks into \u0026ldquo;high\u0026rdquo;, \u0026ldquo;medium\u0026rdquo;, and \u0026ldquo;low\u0026rdquo; priority. High priority tasks were done first as they were key to progress with my dissertation. Once those were done, I moved on to lower priority tasks. I found keeping a \u0026ldquo;low\u0026rdquo; priority list helpful to note down \u0026ldquo;nice to have\u0026rdquo; features that I could come back to at the end.\nI kept track of my goals with Apple Notes, but I know many people like using special to-do list apps or keeping track of tasks in a notebook. Whatever works for you is fine, as long as you use it.\n2) Regularly ask for feedback üó£ This one is super important and I feel like many students don\u0026rsquo;t do it enough. Whenever you can, solicit feedback on your project. It will help you stay on track and give you a fresh perspective on your work so far, ensuring that you\u0026rsquo;re making the most of your time and aren\u0026rsquo;t missing anything.\nThis should be done regularly during meetings with your supervisor. If your supervisor is reluctant or can\u0026rsquo;t give you anything, ask them to think about it for next time.\nIf you ever need to prepare a presentation on your project, make sure to take the feedback of any parties on board and directly address it when you\u0026rsquo;re presenting your work at a later time. When preparing the presentation, practice by presenting it to your friends or family and gather their feedback. It will go a long way.\nGathering feedback will also be important once you\u0026rsquo;ve written up your dissertation. You could swap dissertations with other students and give each other feedback. This can help you as it will expose you to aspects that you may have missed, and it will get you to think about your dissertation from a more critical angle.\n3) Make the most of resources provided to you üíª There are a lot of resources available to students writing their dissertations. This is not something I felt in previous years of university, and it has become very apparent over the last year. Whenever I needed any equipment (and I need a bunch of specialised 4G equipment), I asked my supervisor. He usually managed to sort something out promptly, or if he couldn\u0026rsquo;t then he referred me to someone that could help.\nUniversities generally also have lots of online resources that you have access to. Namely, there is a large collection of books I could access online. Those helped greatly with obtaining technical diagrams and writing up the background section for my dissertation.\nIf your university provides example dissertations from previous years or mark schemes, use them! Especially if you\u0026rsquo;re able to find a highly-graded dissertation similar to your dissertation topic, use it as inspiration as to how to structure your dissertation and what to include in it.\n4) Keep a record of all you\u0026rsquo;ve done ‚úÖ It\u0026rsquo;s very useful to make a note of what work you\u0026rsquo;ve done and the details of it. It especially comes in handy when writing up the dissertation and trying to remember the details of something you\u0026rsquo;d have done months ago.\nIt\u0026rsquo;s also useful when discussing progress in weekly supervisor meetings. Your supervisor might as you things that you can\u0026rsquo;t remember off the top of your head, so having what you did noted down helps in such cases.\n5) If you\u0026rsquo;re stuck, try everything you can and be honest üôã‚Äç‚ôÇÔ∏è Dissertations are difficult. There were many points over the year where I felt like I hit a wall. In times like this, I made it a point to instead try to do anything that could help (even if I thought it\u0026rsquo;s unlikely to work) and noted down what I tried and what the outcomes were. In many cases, I could get closer to the solution through this methodological trial-and-error. If I couldn\u0026rsquo;t progress, I made sure to formulate the problem well and explain it to my dissertation supervisor, mentioning everything I tried.\nIf you\u0026rsquo;re stuck, make sure to be upfront and honest about your issues in progress meetings. Your supervisor would then be aware of these issues early and will be able to help you in a timely manner. In the end, they want you to succeed, so there\u0026rsquo;s no shame in asking for help.\nIt\u0026rsquo;s also useful to ask any technical friends about what ideas they may have when you\u0026rsquo;re stuck. I found myself fixating on certain esoteric aspects of my project sometimes, where the solution turned out to be much simpler once I spoke to someone about it.\n6) Manage your time well ‚è± You will have other modules and commitments that you\u0026rsquo;ll need to allocate time for. Therefore, time management is key. You may find that during some weeks you get lots of work on your dissertation, and in other weeks there won\u0026rsquo;t be much. That\u0026rsquo;s completely fine, but it\u0026rsquo;s important to communicate this to your supervisor and in any progress checks.\nPersonally, I did a bulk of the work in the first term, so in the second term, I focussed on aspects that might not work and writing up my dissertation. Also when I knew that an assignment is coming up from another module, I made sure to focus on my dissertation beforehand. Then, once the assignment was available, I didn\u0026rsquo;t have to focus on my dissertation knowing that I worked on it additionally at another time.\n7) Don\u0026rsquo;t lose sight of the big picture ü•á Continuously remind yourself of what the end goal is of what you\u0026rsquo;re trying to do. I did a security project where I was being marked on the accuracy and thoroughness of my methods. Nevertheless, I found myself spending hours employing various software engineering methods and making my code look good and maintainable. This wasn\u0026rsquo;t necessary and if any, I got negligible credit for this. It simply wasn\u0026rsquo;t the goal of the project and that time could have been better allocated elsewhere.\n8) Allocate time to learn about academic writing üìö Writing up a dissertation will likely be quite different from any work you\u0026rsquo;ve done previously. Especially in computer science, where essays and longer pieces of writing aren\u0026rsquo;t common in many modules, it can take some time to get used to.\nYou will need to read a lot of academic papers to get used to the academic writing style. I underestimated quite heavily how much this would take, and resulted in having to re-write some sections of my dissertation when my sense for academic writing improved.\nIt can be useful to keep a small bank of published papers that you like of which you can refer back to. I did this and then picked aspects I liked out of each one and incorporated them into my dissertation. This also ensured that my dissertation read more like a paper, and less like a student essay.\nAlong with reading about academic writing, some time will need to be allocated to getting familiar with academic writing tools. The vast majority of academic papers are written using LaTeX. I\u0026rsquo;d recommend using LaTeX, as your markers are likely to be researchers that also use it and are likely quite fond of it. It also makes your writing look great out of the box. However, sometimes things may break and extra time will need to aspects that just wouldn\u0026rsquo;t break in an ordinary word processor. A good alternative is using a word processor with a LaTeX document theme.\nI used Overleaf as my LaTeX editor, which made working with LaTeX a breeze. I also used Zotero as my bibliography manager. Writefull also came in handy to make my work look more academic and it integrated with Overleaf well. Finally, Grammarly and Hemingway Editor were useful in giving my dissertation an extra level of polish and helped with getting it to read better.\n9) Keep your final write-up concise ‚úçÔ∏è After working on your dissertation for almost a year, you\u0026rsquo;ll find that there is a lot you can write about. I had to do lots of cutting down to reach the 10,000-word limit and wished I\u0026rsquo;d have been much more concise from the start.\nThe hardest bit for me was parting with the misconception that the time spent on a certain task directly correlates to the amount of space it should take up in the write-up. This was plain wrong. I spent weeks at the start simply trying to understand the basics of what I was working on, which eventually seemed quite trivial to me. Towards the end, however, I could spend a couple of days working on something that I could write whole chapters about. It\u0026rsquo;s important to realise how much value each point brings.\nOn the contrary, it\u0026rsquo;s important to be aware of the curse of knowledge. You\u0026rsquo;re likely to be working on a highly specialised topic that\u0026rsquo;s not common knowledge for many computer science researchers. Something trivial to you may not be trivial to others. To help with this, I recommend reading your dissertation in its entirety from time to time and getting other people to read it also.\nIn the end, a longer dissertation won\u0026rsquo;t mean a better dissertation. In a long dissertation, your weak points will likely dilute your strong points, which could make your markers doubt your understanding of the topic. A good rule to follow is that if it doesn\u0026rsquo;t directly improve your project or feed into something that does, cut it out.\n10) Have fun! üòä Your dissertation is a substantial piece of work, and likely the largest piece of academic work you\u0026rsquo;ve undertaken so far. It\u0026rsquo;s also a great opportunity to hone in on one research topic that you\u0026rsquo;re interested in. Therefore, it\u0026rsquo;s important that you have fun along the way and treat it as an opportunity to focus on what you enjoy in computer science!\n","permalink":"https://dominikrys.com/posts/cs-dissertation-tips/","summary":"An important moment has happened recently - I\u0026rsquo;ve graduated with a First Class BSc degree in Computer Science from the University of Birmingham! To my surprise, during the graduation ceremony, I have also been awarded the Distinguished Dissertation Prize 2020/2021 for the highest-graded dissertation in the graduating cohort. This was quite an honour as it came completely unexpected, and it made a lot of the effort that went into writing my dissertation worthwhile.","title":"10 Tips for Getting a First in a Computer Science Dissertation"},{"content":"For part of my bachelor\u0026rsquo;s dissertation, I implemented and executed a bit-flipping attack on encrypted IP packets in LTE networks. The attack was first documented by David Rupprecht et al. in their academic paper \u0026ldquo;Breaking LTE on Layer Two\u0026rdquo;.\nThe attack is possible due to a specification flaw in LTE standards, where IP packets are not integrity protected. Therefore, a man-in-the-middle (MITM) attacker can modify the packets and the receiver will decrypt them successfully since it can\u0026rsquo;t verify the authenticity of the data. This kind of attack is not specific to LTE networks, however, and can be executed in different environments.\nI learned a lot while implementing this attack, which I thought would be worth documenting and sharing through this post. Some of this post will refer to LTE networks specifically, but I will try to keep much of it general. This post includes how to do bitmasking comfortably in C++, how to find packet offsets using appropriate tools, and how to compensate for any checksum errors in IP packets.\nStream Cipher Attack Explanation In most LTE networks, IP packets are encrypted with a stream cipher (AES-CTR), where the encryption algorithm generates streams of bytes called keystreams. To encrypt data, the sender XORs the unencrypted message (the plaintext) with the keystream to obtain an encrypted message (the ciphertext). The ciphertext is then sent to the receiver. The receiver generates the same keystream and can XOR it with the received ciphertext to retrieve the plaintext.\n  As explained previously, a MITM attacker can successfully modify the ciphertext of internet packets in LTE networks such that they decrypt to a related plaintext. This is possible because the encryption algorithm used is a malleable cipher. If the attacker knows part of the plaintext of the encrypted data, they can apply a calculated manipulation mask (also referred to as a bitmask) to the ciphertext such that it decrypts into any chosen plaintext. The mask is calculated by XORing the plaintext with the message that the attacker wants to modify the plaintext to. The mask is then applied by XORing it with the ciphertext.\nThe attack works because data XORed with itself results in all zeroes, and all zeros XORed with any data keeps the data intact. For more detail, I highly recommend reading a short description of how the attack works on Wikipedia.\n An attacker can apply the mask to only a part of the ciphertext, allowing for the attack if the entire plaintext is not known.\nAs an example, in the context of LTE networks, the network provides the IP of the DNS server that connected devices should use for DNS resolution. Therefore, the plaintext of the destination IP of DNS requests is known. A MITM attacker can use this to change the destination IP to an arbitrary address, hijacking the request.\nImplementing the Bit-Flipping Stream Cipher Attack in C++ With the basics out of the way, I will explain how to implement a bit-flipping stream cipher attack where the plaintext is known in C++.\nObtaining Field Offsets The encrypted packets can be encapsulated in various protocols, so the offset at which to apply the bitmask will differ depending on the context. With more common protocols, you can easily find this information on the internet. In other cases, Wireshark and some testing may be needed.\nIn my case of LTE networks and using the C++ srsLTE software suite, I was working with IPv4 packets encapsulated in the LTE PDCP protocol. Obtaining the offset of the destination IP in the IPv4 packet was trivial, but I found Salim Gasmi\u0026rsquo;s Hex Packet Decoder to be an excellent tool to help with this. The PDCP protocol only adds a 2-byte header to the front of IP packets, so I needed to add 2 to the IP packet offset for it to work in the context of PDCP packets.\nTo additionally verify if my assumptions were correct, I checked example packets in Wireshark. This would also likely need to be done when finding the offsets of more exotic, less well-documented protocols. To obtain the example packets, I captured packets in a test setup with known keys such that the packets could be decrypted. Since we\u0026rsquo;re working with a stream cipher, the offsets will be the same whether encryption is enabled or not. In the Wireshark capture, I was able to confirm that adding 2 to the IP packet offset was correct. This is shown below with the 2-byte PDP header but can also be verified by checking individual bytes.\n Applying Bitmasks to Parts of Payload in C++ We now have the required offsets at which to mask parts of our packets. The next step is to apply the bitmask by XORing it with the encrypted payload. In my case of using srsLTE and LTE networks, the payload was represented by a uint8_t array.\nTo comfortably tweak my implementation and test it without having to set up an entire LTE network, I made myself a test script. To work with real data in the test script, I grabbed some example payloads (encrypted and unencrypted) from a test run and serialised them into a hex string. Then, my test script serialised it back into raw bytes when run. The code I personally used is by Robbie Rosati on Stack Overflow.\nTo calculate the bitmask, the known plaintext of the ciphertext is XORed with the plaintext that you would like the ciphertext to decrypt to. In C++ this is greatly eased by using a std::valarray as it\u0026rsquo;s compatible with regular bitwise operations. Then, the mask is XORed with the payload at the appropriate offset. Overall, the entire implementation looks as follows:\nvoid apply_mask(uint8_t* buf, std::valarray\u0026lt;int\u0026gt; mask, int offset) { for (size_t i = offset; i \u0026lt; offset + mask.size(); i++) { buf[i] ^= mask[i - offset]; } } int main() { // De-serialise example payload (for testing only)  std::string buf_str = \u0026#34;80004500004ce33d00007811\u0026#34;; uint8_t buf[buf_str.length() / 2]; object_from_hex(buf_str, \u0026amp;buf); // Store plaintext and the modified plaintext as std::valarray  std::valarray\u0026lt;int\u0026gt; dest_ip_modified{178, 62, 108, 207}; std::valarray\u0026lt;int\u0026gt; dest_ip_plaintext{103, 123, 226, 10}; // Calculate the bitmask  auto ip_mask = dest_ip_plaintext ^ dest_ip_modified; // Store the previously found offset of our known plaintext  int pdu_dst_ip_byte_offset = 18; // Apply the bitmask to the payload  apply_mask(buf, ip_mask, pdu_dst_ip_byte_offset); } Compensating for the IPv4 Checksum In the example above, the destination IP of IPv4 packets is modified. The checksum is left intact, which will most likely be invalid with the new IP address. In effect, these packets will be correctly encrypted, but the network stack at the receiver is likely to discard them.\nFor the receiver to not discard the packets, the checksum will need to be preserved. This can be done by modifying other fields in the packets, whose fields we can reliably predict. To do this, we first need to understand hot IPv4 checksums are calculated. As stated in the Internet Protocol RFC 791:\n \u0026ldquo;The checksum field is the 16-bit ones' complement of the ones' complement sum of all 16-bit words in the header. For purposes of computing the checksum, the value of the checksum field is zero.\u0026rdquo;\n This is quite a mouthful and personally, I found it difficult to grasp without seeing an example. For a great one, I recommend checking the IPv4 header checksum Wikipedia page.\nIn the case of compensating for changes in the destination IP address, this entails calculating the difference between the 16-bit sums of the original and the modified IP addresses and tweaking other fields appropriately. Note that the 16-bit sum will mean that the first and the third octets of each IP address in the sum are shifted left 8 bits in the checksum calculation.\nIn LTE networks, we can reliably predict the TTL of packets sent from some devices to the first mast, as it won\u0026rsquo;t yet be decremented. This gives us the ability to compensate for changes in the first and third octets of the IP. To compensate for changes in the second and fourth octets, I performed an investigation by sending hand-crafted DNS packets with broken fields to a remotely hosted VM and checking if they\u0026rsquo;re received and processed correctly. The aim was to establish which fields are not necessary for routing and could be modified. This was easily done using Scapy. Eventually, I found that the DSCP and ECN fields could be predicted and didn\u0026rsquo;t impact routing, so these gave enough room for me to be able to compensate for changes in the second and fourth octets of the IP.\n IP Checksum Compensation Code Example To verify if my IP checksum compensation code worked, I added an implementation of it to my test script from before.\nTo compensate for changes in the first and third IP octet using the TTL field:\n/* Mask TTL */ // Store the TTL field plaintext int ttl_plaintext = 64; // Calculate the difference in the 16-bit totals of the original and modified destination IPs int dest_ip_plaintext_total = (dest_ip_plaintext[0] \u0026lt;\u0026lt; 8) + dest_ip_plaintext[1] + (dest_ip_plaintext[2] \u0026lt;\u0026lt; 8) + dest_ip_plaintext[3]; int dest_ip_modified_total = (dest_ip_modified[0] \u0026lt;\u0026lt; 8) + dest_ip_modified[1] + (dest_ip_modified[2] \u0026lt;\u0026lt; 8) + dest_ip_modified[3]; int chksm_difference = dest_ip_plaintext_total + (ttl_plaintext \u0026lt;\u0026lt; 8) - dest_ip_modified_total; // Extract only the higher byte from the difference that the TTL field can compensate for int ttl_modified = chksm_difference \u0026gt;\u0026gt; 8; // Assert that the modified TTL field is valid assert(ttl_modified \u0026gt;= 20 \u0026amp;\u0026amp; \u0026#34;TTL under 20: may not reach the destination\u0026#34;); assert(ttl_modified \u0026lt;= 255 \u0026amp;\u0026amp; \u0026#34;TTL over 255: too large\u0026#34;); // Calculate the bitmask int ttl_mask_val = ttl_plaintext ^ttl_modified; std::valarray\u0026lt;int\u0026gt; ttl_mask = {ttl_mask_val}; // Apply the bitmask to the ciphertext int ttl_byte_offset = 10; apply_mask(buf, ttl_mask, ttl_byte_offset); To compensate for changes in the second and fourth IP octet using the DSCP and ECN fields:\n/* Mask DSCP/ECN */ // Store the DSCP/ECN field plaintext int dscp_ecn_plaintext = 0; // Extract only the lower byte from the difference that the DSCP/ECN field can compensate for int dscp_ecn_modified = chksm_difference \u0026amp; 0xFF; // Assert that the modified DSCP/ECN value is valid assert(dscp_ecn_mod \u0026gt;= 0 \u0026amp;\u0026amp; \u0026#34;DSPCP/ECN under 1: invalid\u0026#34;); assert(dscp_ecn_mod \u0026lt;= 255 \u0026amp;\u0026amp; \u0026#34;DSPCP/ECN above 255: invalid\u0026#34;); // Calculate the bitmask int dscp_ecn_mask_val = dscp_ecn_plain ^dscp_ecn_mod; std::valarray\u0026lt;int\u0026gt; dscp_ecn_mask = {dscp_ecn_mask_val}; // Apply the bitmask to the ciphertext int pdu_dscp_ecn_byte_offset = 3; apply_mask(buf, dscp_ecn_mask, pdu_dscp_ecn_byte_offset); Caveats of Stream Cipher Attacks on IP Packets Impossible to Compensate for Changes You may that it won\u0026rsquo;t be possible to compensate for some changes in the fields, especially to the higher byte of the 16-bit sum. The lower byte can always carry into the higher byte, but the higher byte doesn\u0026rsquo;t have this luxury. If it\u0026rsquo;s not possible to find enough fields whose plaintext can be predicted, it may not be possible to compensate for any changes unless some external changes are made.\nThis is an issue that I have encountered, where the destination IP couldn\u0026rsquo;t be compensated for. In my case, I was redirecting DNS packets to a remote VM that I controlled, so I had control over the modified destination IP. I fixed this issue by deploying a VM hosted by another provider or in another region, such that the IP would differ adequately.\nIncorrect UDP/TCP Checksums Another issue that could occur is that the UDP checksum will be incorrect, causing the receiver to discard the packet. This occurs because the UDP checksum is calculated separately from the IP checksum. If you control the receiver, this issue could be alleviated by ignoring UDP checksums. There is conflicting advice on how to do this online, so I wrote a guide on how to disable UDP checksum validation in Linux which you can find here.\nI\u0026rsquo;ve attempted to correct the UDP checksum by masking other fields in the packet as well. Below is a diagram of all the fields that are used to calculate the UDP checksum, where the red fields are from the IPv4 packet:\n After modifying every non-payload field to see if the packets will be received, I found that Linux rejected them at the kernel level. The only promising change that worked was when part of the payload could be predicted and modified. Otherwise, pervasive changes would need to be made to the IP stack such that any incorrect fields are ignored and corrected.\n","permalink":"https://dominikrys.com/posts/stream-cipher-attacks-on-ip/","summary":"For part of my bachelor\u0026rsquo;s dissertation, I implemented and executed a bit-flipping attack on encrypted IP packets in LTE networks. The attack was first documented by David Rupprecht et al. in their academic paper \u0026ldquo;Breaking LTE on Layer Two\u0026rdquo;.\nThe attack is possible due to a specification flaw in LTE standards, where IP packets are not integrity protected. Therefore, a man-in-the-middle (MITM) attacker can modify the packets and the receiver will decrypt them successfully since it can\u0026rsquo;t verify the authenticity of the data.","title":"Executing Stream Cipher Attacks on IP Packets"},{"content":"I wanted to learn Rust for a while. The promises of memory-safety and performance have piqued my interest and I needed to see what all the hype was about. In turn, I spent some time learning Rust on and off over the last couple of months alongside university work.\nIn this post, I describe how I went about learning Rust and what resources I used. I also reflect on what methods worked well and what could have possibly been skipped. I hope this will be useful to anyone that\u0026rsquo;s also curious about Rust.\nOverall, I started learning Rust by first working through the official Rust book, then working through the Rust Exercism exercises, and finally working on a project. In this post, I will describe each of the mentioned resources in further depth. I hope you enjoy the post!\nThe Rust Book When researching a starting point for learning Rust, I found that the near-unanimous answer is to read the Rust Programming Language book. Although this book is available in physical and e-book forms, the most popular way to read it is in a website format.\n The book contains up-to-date information with stable Rust and is actively updated. It covers the most important concepts of the language and even includes some projects within it, which massively helps with getting hands-on experience.\nOverall the book was a great introduction to Rust and is very approachable. However, the book covers plenty of information and can take some time to work through. Personally, I found that I spent too much time reading about the theory and not enough time using the knowledge or assimilating it in any meaningful manner. Therefore, I feel that I shouldn\u0026rsquo;t have dwelled too hard on some of the less common concepts and started on projects sooner instead.\nExercism After finishing the Rust book, I started the Exercism Rust track. I\u0026rsquo;ve been told about Exercism by a co-worker and have been eager to try it out since.\n Overview Effectively, Exercism provides programming puzzles in increasing difficulty, structured into \u0026ldquo;tracks\u0026rdquo;. Many languages are supported, one of which is Rust. The site provides a testing framework that you set up locally, which you then use to download exercises and upload solutions to the website.\nOn Exercism, a track contains core and extra exercises that you can work through in mentored or practice mode. In mentored mode (which is recommended), you complete the core exercises one by one. Then, a mentor gives feedback on your answers and allows you to then progress to the next exercise. When you complete a core exercise you also unlock some of the extra exercises, which mentors can also give feedback on (but I found they rarely do). In practice mode everything is unlocked outright, but mentor feedback is disabled. Also, when you finish a question in either mode you can publish your answer for others to check out and comment on.\nPersonal Experience I found this to be a great transition from the Rust book as it requires you to know the basics of the language that you are coding in. To complete each exercise successfully independent research is required, which allows you to learn the language in an autonomous manner.\nAs to the website\u0026rsquo;s features, I found being able to see other people\u0026rsquo;s solutions very useful. After I came up with a solution, I checked other people\u0026rsquo;s to get inspiration from and used them to improve my own. This also greatly helped with learning how to write idiomatic Rust. Posting my own solutions was also massively beneficial. I had many people comment on my solutions, prompting interesting conversations and making me think about my code at a deeper level.\nSince exercises are pulled down and solved locally, Exercism exercises lend themselves to creating a sort of \u0026ldquo;solution bank\u0026rdquo;. This allows for setting up Git in your Exercism directory, allowing for version control as you improve your answers. It also provides a great source of reference - for example, my solutions are available as a repo on my GitHub, which I found myself revisiting when working on other projects.\nI read some criticisms of people saying that it takes a long time to received feedback in mentored mode and to switch to practice mode instead. Personally, I found this to not entirely be the case. Sometimes I could get feedback a couple of hours after submitting my solution, and in the worst case, I had to wait 2-3 days. The actual mentor feedback varied greatly in quality, however. Many mentors suggested improvements that I could find by briefly looking at other people\u0026rsquo;s solutions. If I already implemented improvements inspired by other solutions, many mentors didn\u0026rsquo;t have much to comment on. The best mentors benchmarked my solution against other solutions and suggested areas of improvement. I could then delve deep into the performance of my code and what Rust was doing under the hood\nOverall, this was a good next step after reading the Rust book. It allowed me to do a lot of my own research on Rust with less hand-holding than the Rust book. While solving the exercises, I could also use many of the concepts described in Rust by Example.\nProject After finishing many Exercism exercises, I applied my knowledge to a project. I did this by checking the build-your-own-x GitHub repo which contains a wealth of tutorials and guides for different projects. There are a couple of interesting projects using Rust there, from which I chose Emil Hernvall\u0026rsquo;s guide on writing a DNS server in Rust.\nCompared to Exercism and the Rust book, I learned a lot about how to structure and work on a Rust project from this guide. It also allowed me to get familiar with Rust tooling (e.g. testing and linting). Furthermore, there was plenty that could be improved upon the code in the guide as it was written a couple of years ago, and overall describes a fairly simple and bare-bones DNS server implementation. I found applying what I learned from Exercism exercises to improve upon the code in the guide particularly redeeming.\nThe final DNS server I wrote is available on my GitHub.\nWhat Next? I still have a lot of Rust to learn, and I\u0026rsquo;m hoping to expand my knowledge by working on other projects. So far, I thoroughly enjoyed learning the language, even despite its somewhat steep learning curve. Nevertheless, I\u0026rsquo;m excited to see it being adopted more in industry on major projects, especially with claims that it can be as productive as higher-level languages.\n","permalink":"https://dominikrys.com/posts/learning-rust/","summary":"I wanted to learn Rust for a while. The promises of memory-safety and performance have piqued my interest and I needed to see what all the hype was about. In turn, I spent some time learning Rust on and off over the last couple of months alongside university work.\nIn this post, I describe how I went about learning Rust and what resources I used. I also reflect on what methods worked well and what could have possibly been skipped.","title":"How I Started Learning Rust"},{"content":"I recently needed to disable the validation of UDP checksums of incoming packets on a Linux machine for a security project. To my surprise, there weren\u0026rsquo;t any satisfactory solutions that I could easily find online related to this. The top results also suggested disabling checksum offloading, which doesn\u0026rsquo;t disable checksum checking. In the end, I managed to figure this problem out and found that it\u0026rsquo;s possible without recompiling the kernel. In this short post, I\u0026rsquo;ll describe how to set up a Linux machine to ignore UDP checksums in received packets. The mentioned steps may also be adapted to allow for disabling TCP checksum checking.\nCheck if your machine can receive packets with broken UDP checksums Firstly, we need to check if your machine can already accept packets with invalid UDP checksums. Testing this is easy - send packets with broken UDP checksums from one machine (machine 1) to the machine that you want to disable validation on (machine 2), and check the traffic using tcpdump. I\u0026rsquo;ll outline how I\u0026rsquo;ve done this below.\n  Run tcpdump on machine 1, listening to internet traffic at port 53:\nsudo tcpdump -i \u0026lt;NETWORK INTERFACE\u0026gt; dst port 53 -vv To get the network interface names, you can run ip link show.\n  Disable transmit checksum offloading on machine 1. This is so that any invalid checksums won\u0026rsquo;t be corrected by the hardware. In some cases, it may not be possible to disable this, so another machine may need to be used. To disable transmit checksum offloading on Linux, run:\nsudo ethtool --offload \u0026lt;NETWORK INTERFACE\u0026gt; tx off   Download and run Scapy on machine 2.\n  Craft a DNS packet with a broken UDP checksum using Scapy on machine 2:\nbad_packet = IP(dst=\u0026#39;\u0026lt;MACHINE 1 IP\u0026gt;\u0026#39;) / UDP() / DNS(rd=1, qd=DNSQR(qname=\u0026#34;www.example.com\u0026#34;)) Make sure to replace \u0026lt;MACHINE 1 IP\u0026gt; with the IP of machine 1.\n  Send the packet with a broken checksum from machine 2 to machine 1:\nsend(bad_packet)   Check the tcpdump logs on machine 1. If the packet is received stating bad udp cksum in the logs, the machine can receive packets with broken UDP checksums. We can then continue with adding rules to ignore the checksums.\n  What if my machine doesn\u0026rsquo;t receive packets with invalid UDP checksums? A router between your machines could discard the packet due to an incorrect UDP checksum. Such an issue can be hard to diagnose, so it may be circumvented by sending packets from another machine.\nIf the packet is still not received, the kernel may be rejecting packets with invalid UDP checksums. In such case, the udp_recvmsg() function in the kernel would need to be modified to not return errors when the checksum validation fails. However, changes to the kernel were not needed on the machines that I have tested this on (Ubuntu 18.04 in Microsoft Azure, Ubuntu 20.10 in DigitalOcean, and Arch Linux with kernel version 5.11.11).\nIgnoring UDP checksums with nftables So far, we\u0026rsquo;ve confirmed that packets with broken UDP checksums can be received by our machine. However, these packets won\u0026rsquo;t get accepted by any target applications due to the invalid checksum. We can fix this using nftables.\nWe can configure nftables rules that set the UDP checksum of received packets to 0 before they\u0026rsquo;re passed to any applications. Packets with UDP checksums of 0 will not have their checksums validated, effectively disabling UDP checksum validation.\nTo set this up, first install nftables with your favourite package manager. Next, add the following nftables rule:\nsudo nft add table input_table sudo nft \u0026#39;add chain input_table input {type filter hook input priority -300;}\u0026#39; sudo nft \u0026#39;add rule input_table input ip protocol udp udp checksum set 0\u0026#39; This rule will set the UDP checksum of every received IP UDP packet to 0. Your machine will now ignore UDP checksums of received packets! Feel free to test it using Scapy.\nTo make the rule persistent across reboots, I\u0026rsquo;d recommend reading through this short guide on nftables.\nIgnoring UDP checksums using socket options If you have the source code of the application that you want to send the packets with broken UDP checksums to, it may be possible by using socket options. To do so, the SO_NO_CHECK option would need to be declared with the UDP socket file descriptor, as described here.\n","permalink":"https://dominikrys.com/posts/disable-udp-checksum-validation/","summary":"I recently needed to disable the validation of UDP checksums of incoming packets on a Linux machine for a security project. To my surprise, there weren\u0026rsquo;t any satisfactory solutions that I could easily find online related to this. The top results also suggested disabling checksum offloading, which doesn\u0026rsquo;t disable checksum checking. In the end, I managed to figure this problem out and found that it\u0026rsquo;s possible without recompiling the kernel. In this short post, I\u0026rsquo;ll describe how to set up a Linux machine to ignore UDP checksums in received packets.","title":"How to Disable UDP Checksum Validation in Linux"},{"content":"I\u0026rsquo;ve recently worked on a security project which required me to transparently/interceptingly (if that\u0026rsquo;s a word) proxy IP packets that have had their destination IPs spoofed. By this, I mean that the destination IP in an IP packet is not the IP of the destination which a DNS request would correctly resolve. For example, this could be due to a DNS query being spoofed and sending an IP address of another destination in reply.\nIn this post, I will explain how it\u0026rsquo;s possible to proxy such HTTP traffic by redirecting it to the correct destination.\nWhy can\u0026rsquo;t spoofed DNS packets be proxied using an ordinary transparent proxy? When an IP packet with a spoofed destination IP reaches its destination server, the server will handle it like any other IP packet that has been destined for it. Ordinary transparent proxying tools such as Squid are usually configured as internet gateways, so they are not the final destination of the IP packets that pass through them. Since the final destination IP of each packet is known in such setups, these tools can easily send packets to their destination. If the destination IP is the proxy, as it would be in the case of spoofed destination IPs, the transparent proxy would need to additionally resolve the original destination IP. Most such tools don\u0026rsquo;t have support for this.\nHow to reclaim the original destination? To reclaim the original destination, the Host header can be used which is required by HTTP/1.1. The Host header contains the domain name that the client wants to access. In the case of spoofed destination IPs, this header will be intact and pointing to the un-spoofed destination.\nTo reclaim the original destination, proxy software is needed that can do a DNS lookup on the Host header and send traffic to the destination resulting from the lookup. Sadly, in this case, the most popular transparent proxy software won\u0026rsquo;t work. I tried extensively to make this work with Squid, but there are some reasons why it\u0026rsquo;s not possible (more here and here).\nIt\u0026rsquo;s also worth mentioning that since this proxy will change the destination IP of the packet, it stops being \u0026ldquo;transparent\u0026rdquo;, and is now an \u0026ldquo;intercepting\u0026rdquo; proxy. I thought that this term is reserved for slightly more involved proxies such as Burp, but it also applies in this case.\nTo resolve packets according to their Host header, I used Privoxy in intercepting mode, which I will explain how to configure.\nHow to configure Privoxy to resolve spoofed IP packets? Luckily the configuration for Privoxy is very simple. First, install it using your operating system\u0026rsquo;s package manager. Next, modify its configuration file under etc/privoxy/config with the following details, where INTERFACE_IP is the IP of the interface that you want Privoxy to listen at:\nlisten-address INTERFACE_IP:3128 accept-intercepted-requests 1 debug 1 Note that debug 1 is not strictly needed, but it will allow us to see if requests are coming through to the server. accept-intercepted-requests 1 is the important part, which enabled the \u0026ldquo;intercepting\u0026rdquo; mode of Privoxy.\nNext, restart privoxy:\nsudo systemctl restart privoxy Finally, add an iptables rule to redirect traffic from the HTTP port to the port that Privoxy is listening at:\nsudo iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 8118 You can modify the above rule or add additional ones if you have other interfaces apart from eth0 that you\u0026rsquo;d like to forward traffic from.\nNow you can send some spoofed requests to the server and check if you can see them in the logs:\n$ sudo tail -f /var/log/privoxy/logfile 2021-04-15 15:06:24.434 7f39feffd700 Request: scratchpads.org/ 2021-04-15 15:06:24.789 7f39feffd700 Request: scratchpads.org/css/main.css 2021-04-15 15:06:24.795 7f39dffff700 Request: scratchpads.org/css/index.css 2021-04-15 15:06:24.931 7f39ff7fe700 Request: work.a-poster.info:25000/ 2021-04-15 15:06:26.503 7f39dffff700 Request: scratchpads.org/assets/why/accordion.js 2021-04-15 15:06:26.554 7f39feffd700 Request: scratchpads.org/assets/index.js HTTPS support? The method described in this post won\u0026rsquo;t work for HTTPS requests, since the HTTP header will be encrypted and the Host header won\u0026rsquo;t be able to be read. As far as I know there are no tools available that would be able to do this. In theory, it\u0026rsquo;s possible to make this work with HTTPS in a transparent manner, but with substantial engineering effort.\nI\u0026rsquo;m envisioning a solution where the spoofing DNS server redirects each request to a different machine, where each machine knows about the original destination of the packets. The machines can then redirect packets to their correct destinations. The contents will still be encrypted, so it\u0026rsquo;s questionable whether something like this would be worth doing at all.\n","permalink":"https://dominikrys.com/posts/transparently-proxy-spoofed-ip/","summary":"I\u0026rsquo;ve recently worked on a security project which required me to transparently/interceptingly (if that\u0026rsquo;s a word) proxy IP packets that have had their destination IPs spoofed. By this, I mean that the destination IP in an IP packet is not the IP of the destination which a DNS request would correctly resolve. For example, this could be due to a DNS query being spoofed and sending an IP address of another destination in reply.","title":"How to Transparently Proxy IP Packets With Spoofed Destinations"},{"content":"I\u0026rsquo;ve recently set up Squid as a transparent proxy for a security project. What should have been relatively straightforward had me browsing through tutorials from over 10 years ago that don\u0026rsquo;t quite work any more. After comparing this prehistoric knowledge with some supposedly up-to-date documentation, I managed to understand enough about Squid to get a minimal transparent proxy configuration on a modern version of Linux hosted in the cloud.\nWith the hopes of saving someone some time that may be embarking on a similar journey, I thought I\u0026rsquo;d write this post. We discuss HTTP transparent proxying at the start, but provide resources for allowing support for HTTPS.\nThe following instructions have been tested on Ubuntu 18.04 deployed in Azure, and Ubuntu 20.04 on DigitalOcean.\nInstalling Squid This part is straightforward, so just follow the normal install procedure for your operating system/package manager. I used Ubuntu, so installing Squid was as easy as sudo apt install squid.\nBefore we continue, it\u0026rsquo;s worth checking if Squid is able to run at this point (which may not be the case if something is using Squid\u0026rsquo;s default port, for example). It should be running after installation, which you can check with systemctl status squid. If squid is not running, try to fix anything at this point.\nConfiguring Squid Configuration file Now the most important part - the configuration. The config is stored under /etc/squid/squid.conf, but before we make any changes I like to make a copy of the original:\nsudo cp /etc/squid/squid.conf /etc/squid/squid.conf.orig Next, edit the configuration file with your favourite text editor:\nsudo vim /etc/squid/squid.conf And enter this minimal configuration:\nhttp_access allow all http_port 3128 intercept The http_access parameter should ideally be narrowed down as described in the Squid documentation, but to eliminate potential errors we will permit anything to access the proxy.\nThe http_port states which port Squid will listen at, for which we keep the default 3128. We will redirect traffic to this port using iptables soon. intercept is needed to make Squid act as a transparent proxy.\nNothing else is necessary for a working configuration as of the time of writing this post, unlike what some other tutorials may lead you to believe. Note that in its current state, there will be a warning printed in the Squid logs whenever it\u0026rsquo;s started, stemming from the fact that a non-transparent port is not open. If you\u0026rsquo;d like to silence that, you can have Squid listen at a vacant port by adding e.g. http_port 3129 to the configuration.\nFinally, we can restart Squid:\nsudo systemctl restart squid This should be it for the Squid configuration! Make sure to check if it\u0026rsquo;s working, as described earlier in the post. If it\u0026rsquo;s not, good places to start are the journalctl entries for squid, and the access and log files by default located at /var/log/squid/access.log and /var/log/squid/cache.log, respectively.\nEnabling IP forwarding Since we\u0026rsquo;re configuring a transparent proxy, we need to configure IP forwarding on the system:\nsudo sysctl net.ipv4.ip_forward=1 To make this configuration persistent, modify /etc/sysctl.conf and uncomment the line:\n#net.ipv4.ip_forward=1 iptables Rules To get the kernel to forward packets received at port 80 to Squid, we need the following iptables rule:\nsudo iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 3128 Make sure to modify the above rule, or add additional ones if you have other interfaces apart from eth0 that you\u0026rsquo;d like to forward traffic from. Those can be found using e.g. ip link show or ifconfig. This rule makes it so that only external traffic will be send to Squid, and all traffic originating at the machine will reach its destination and not cause a cycle.\nIf at any point you make a mistake with your configuration, you can flush all existing iptables NAT rules:\nsudo iptables -t nat -F Or list any existing rules using:\nsudo iptables -t nat -L Closing Notes - HTTPS Support, Gateway Setup, Spoofed Requests You should now have a minimal Squid transparent proxy running. Make sure to configure the machine as the default gateway for whichever machines you\u0026rsquo;d like to transparently proxy data for.\nTo enable transparent proxying of HTTPS traffic, I recommend suntong\u0026rsquo;s guide.\nNote that Squid is unable to resolve the original destination of packets that have had their destination IP spoofed (source). To resolve those properly, I\u0026rsquo;ve had luck using Privoxy in intercepting mode as I describe in this post.\nThanks for reading, and I hope that this post helped anyone struggling with Squid!\n","permalink":"https://dominikrys.com/posts/squid-transparent-proxy/","summary":"I\u0026rsquo;ve recently set up Squid as a transparent proxy for a security project. What should have been relatively straightforward had me browsing through tutorials from over 10 years ago that don\u0026rsquo;t quite work any more. After comparing this prehistoric knowledge with some supposedly up-to-date documentation, I managed to understand enough about Squid to get a minimal transparent proxy configuration on a modern version of Linux hosted in the cloud.\nWith the hopes of saving someone some time that may be embarking on a similar journey, I thought I\u0026rsquo;d write this post.","title":"A Modern Way to Configure Squid as a Transparent Proxy"},{"content":"I\u0026rsquo;ve recently been working extensively with srsLTE for my bachelor\u0026rsquo;s dissertation. So far, the greatest difficulty has been debugging the software. In this short post, I will describe various ways I found that srsLTE can be debugged, and any pitfalls that come with them.\nI\u0026rsquo;ll assume you know how to debug ordinary C/C++ programs (I\u0026rsquo;ll patiently wait here if you need to have a look into that).\nCompiling srsLTE in debug mode Your first attempt at debugging may have been to compile with the Debug CMake flag, and then executing the binaries using GDB or another debugger:\ncmake ../ -DCMAKE_BUILD_TYPE=Debug This will probably work for srsEPC, but you may have trouble with srsENB and srsUE as the code most likely won\u0026rsquo;t get past the Random Access Procedure (RAP). Due to how time-sensitive the RAP is, binaries in Debug mode are too slow and are unable to complete the procedure. However, if what you need to debug occurs before the RAP, this method will most likely be sufficient.\nAlternatively, srsLTE can be built in release with debug info mode, which will reliably get past the RAP:\ncmake ../ -DCMAKE_BUILD_TYPE=RelWithDebInfo An issue with this is that plenty of code will be optimised out, so you may not hit the breakpoints you need. Sporadic segmentation faults may also occur, so your mileage may vary.\nAs an extra caveat, using SDRs while running srsLTE may not be possible due to the extra latency introduced by the debugger. I\u0026rsquo;ve tried using the Ettus Research USRP B200 and B210 while debugging, and the UHD driver constantly times out for both. Instead, the ZeroMQ driver will most likely need to be used while debugging.\nWhen Release With Debug Info doesn\u0026rsquo;t work For many issues, I had to resort to print statements. This is just about good enough for most issues, especially combined with srsLTE logging if it\u0026rsquo;s cranked up to the debug level.\nIncreasing all_hex_limit inside the .conf files from 32 to something greater can also help if you\u0026rsquo;re inspecting various messages/objects, as it will allow for more hex to be printed in the logs.\nPrinting hex in the console To ease debugging, a quick hack can be added to srsLTE for allowing objects to be printed in the console. This is particularly useful for printing certain PDUs of interest. To achieve this in srsLTE 20.04.2, add following code to log_filter.cc:\nvoid log_filter::console_hex(const uint8_t* hex, int size) { console(hex_string(hex, size).c_str()); } The appropriate headers will also need to be changed in log.h and log_filter.h. Objects can then by printed as hex by calling:\nlog-\u0026gt;console_hex(pdu-\u0026gt;msg, pdu-\u0026gt;N_bytes); Successful debugging in a VM If debugging in debug mode is really necessary, A VM can be used as a last resort. I found that it\u0026rsquo;s possible there to get past the RAP with srsLTE built in Debug mode. Note that SDRs will most likely not work due to the latency introduced by the VM, in which case the srsLTE ZeroMQ driver will need to be used instead.\n","permalink":"https://dominikrys.com/posts/debug-srslte/","summary":"I\u0026rsquo;ve recently been working extensively with srsLTE for my bachelor\u0026rsquo;s dissertation. So far, the greatest difficulty has been debugging the software. In this short post, I will describe various ways I found that srsLTE can be debugged, and any pitfalls that come with them.\nI\u0026rsquo;ll assume you know how to debug ordinary C/C++ programs (I\u0026rsquo;ll patiently wait here if you need to have a look into that).\nCompiling srsLTE in debug mode Your first attempt at debugging may have been to compile with the Debug CMake flag, and then executing the binaries using GDB or another debugger:","title":"How to Debug srsLTE"},{"content":"Motivation During my last internship, I\u0026rsquo;ve been tasked with designing and deploying infrastructure for monitoring a cluster of machines that were used for performance testing. I wrote a blog post detailing high-level choices about it which you can check out here. The post also includes justifications for why I chose to deploy everything in Docker, and why I chose to work with Grafana and InfluxDB as the front-end and time-series database, respectively.\nIt\u0026rsquo;s relatively straightforward to write and deploy a Docker compose application with just Grafana and InfluxDB. There are many ready-made docker-compose.yml files that can be found online, as well as various tutorials and blog posts which explain the details. The main difficulty was in getting the application secured by issuing and renewing TLS certificates. My initial idea was to manually issue and set TLS certificates as described e.g. in the InfluxDB documentation, but this kind of approach wouldn\u0026rsquo;t be maintainable in the long run.\nThis is where Traefik came in - it\u0026rsquo;s an edge router which acts as a reverse proxy into your Docker compose application. More importantly, it aims to \u0026ldquo;make networking boring\u0026rdquo;, which in our case is achieved by automatically issuing and renewing TLS certificates from Let\u0026rsquo;s Encrypt - perfect! I\u0026rsquo;ll describe how to set it up in this post.\nThroughout the post, I\u0026rsquo;ll also describe small improvements that could be made to the deployment I describe, as well as describe small quirks I found when working with the described tools. Note that Traefik works with all Docker containers, so this post can still apply if you for example use Prometheus instead of InfluxDB as your time-series database.\nFor the finished all-in-one deployment of Grafana, InfluxDB and Traefik that this post will build up to, I\u0026rsquo;ve provided this GitHub repo: Secure Monitoring Solution in Docker.\nSetting up Grafana and InfluxDB Setting up Grafana and InfluxDB on Docker is pretty straightforward. Here\u0026rsquo;s a docker-compose.yml that will do just that:\nversion: \u0026#34;3.8\u0026#34; services: influxdb: container_name: influxdb image: influxdb:1.8.3-alpine ports: - 8086:8086 volumes: - influxdb-data:/var/lib/influxdb environment: INFLUXDB_DB: example_db INFLUXDB_ADMIN_USER: admin INFLUXDB_ADMIN_PASSWORD: influxdb-admin networks: - monitoring grafana: container_name: grafana image: grafana/grafana:7.3.4 ports: - 3000:3000 volumes: - grafana-data:/var/lib/grafana environment: GF_SECURITY_ADMIN_USER: grafana GF_SECURITY_ADMIN_PASSWORD: grafana-admin networks: - monitoring depends_on: - influxdb networks: monitoring: driver: bridge volumes: influxdb-data: external: false grafana-data: external: false This simple Docker compose application creates Grafana and InfluxDB containers, creates persistent volumes for them so data will be saved if they\u0026rsquo;re restarted, and sets up a networking bridge so the containers can communicate between each other (in favour of the legacy links command that some old setups use).\nTo start up the containers, run:\ndocker-compose up Grafana can then be accessed at http://localhost:3000 and InfluxDB at http://localhost:8086. You can go ahead and send some data to InfluxDB (e.g. with Telegraf) to make sure it works. InfluxDB will need to be set up as a data source manually in Grafana, however it can be automated as I\u0026rsquo;ll describe later.\nThe database specified by the INFLUXDB_DB environment variable will be created when InfluxDB is initialised. This is poorly documented sadly, but most of the details are included in this InfluxDB PR.\nThe admin account details are provided in the docker-compose.yml above and should be changed to something secure. For ideas on how to store them securely, I recommend looking at Docker secrets or Ansible Vault.\nAdditional configuration I\u0026rsquo;ve decided to use the alpine image variant for InfluxDB, mostly due to its smaller footprint. It\u0026rsquo;ll be sufficient for most uses. Grafana uses an Alpine base image by default, and that\u0026rsquo;s also the one that\u0026rsquo;s recommended. The \u0026ldquo;Image Variants\u0026rdquo; section of the InfluxDB docker image documentation explains the image variants in more depth.\nThis kind of configuration is the foundation of your Grafana and InfluxDB monitoring setup. For now, feel free to change any settings using the appropriate environment variables - I found this way of configuring containers to be much cleaner than mounting a config file into the container, especially once we add Traefik with its labels to the deployment.\nTo configure Grafana and InfluxDB using environment variables:\n  InfluxDB: the format for the environment variables is INFLUXDB_SECTION_NAME, and all dashes (-) are replaced with underscores (_). Certain settings don\u0026rsquo;t have sections, in which case the section part can be omitted.\n  Grafana: the format for the environment variables is GF_SECTION_NAME, where full stops (.) and dashes (-) should be replaced by underscores (_).\n  Securing containers using Traefik Initially, I found how to secure the described Docker compose deployment with Traefik from Jan Grzegorowski\u0026rsquo;s blog post. I highly recommend reading his post if anything is not clear here (I won\u0026rsquo;t be offended).\nAs described already, Traefik is a cloud-native edge router which will serve as a reverse proxy in our Docker compose application. It will automatically issue and renew TLS certificates, so the traffic to and from our Docker containers will be encrypted. Luckily it\u0026rsquo;s also quite straightforward to configure using labels, and the entire configuration can be kept inside one docker-compose.yml file.\nAdapting the above example docker-compose.yml to work with Traefik, we get:\nversion: \u0026#34;3.8\u0026#34; services: influxdb: container_name: influxdb image: influxdb:1.8.3-alpine volumes: - influxdb-data:/var/lib/influxdb networks: - monitoring labels: - \u0026#34;traefik.http.routers.influxdb-ssl.entryPoints=influxdb-port\u0026#34; - \u0026#34;traefik.http.routers.influxdb-ssl.rule=host(`YOUR_DOMAIN`)\u0026#34; - \u0026#34;traefik.http.routers.influxdb-ssl.tls=true\u0026#34; - \u0026#34;traefik.http.routers.influxdb-ssl.tls.certResolver=lets-encrypt-ssl\u0026#34; - \u0026#34;traefik.http.routers.influxdb-ssl.service=influxdb-ssl\u0026#34; - \u0026#34;traefik.http.services.influxdb-ssl.loadBalancer.server.port=8086\u0026#34; grafana: container_name: grafana image: grafana/grafana:7.3.4 volumes: - grafana-data:/var/lib/grafana networks: - monitoring depends_on: - influxdb labels: - \u0026#34;traefik.http.routers.grafana.entryPoints=port80\u0026#34; - \u0026#34;traefik.http.routers.grafana.rule=host(`YOUR_DOMAIN`)\u0026#34; - \u0026#34;traefik.http.routers.grafana.middlewares=grafana-redirect\u0026#34; - \u0026#34;traefik.http.middlewares.grafana-redirect.redirectScheme.scheme=https\u0026#34; - \u0026#34;traefik.http.middlewares.grafana-redirect.redirectScheme.permanent=true\u0026#34; - \u0026#34;traefik.http.routers.grafana-ssl.entryPoints=port443\u0026#34; - \u0026#34;traefik.http.routers.grafana-ssl.rule=host(`YOUR_DOMAIN`)\u0026#34; - \u0026#34;traefik.http.routers.grafana-ssl.tls=true\u0026#34; - \u0026#34;traefik.http.routers.grafana-ssl.tls.certResolver=lets-encrypt-ssl\u0026#34; - \u0026#34;traefik.http.routers.grafana-ssl.service=grafana-ssl\u0026#34; - \u0026#34;traefik.http.services.grafana-ssl.loadBalancer.server.port=3000\u0026#34; traefik: container_name: traefik image: traefik:v2.3.4 volumes: - traefik-data:/letsencrypt - /var/run/docker.sock:/var/run/docker.sock networks: - monitoring ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; - \u0026#34;8086:8086\u0026#34; command: - \u0026#34;--providers.docker=true\u0026#34; - \u0026#34;--entryPoints.port443.address=:443\u0026#34; - \u0026#34;--entryPoints.port80.address=:80\u0026#34; - \u0026#34;--entryPoints.influxdb-port.address=:8086\u0026#34; - \u0026#34;--certificatesResolvers.lets-encrypt-ssl.acme.tlsChallenge=true\u0026#34; - \u0026#34;--certificatesResolvers.lets-encrypt-ssl.acme.storage=/letsencrypt/acme.json\u0026#34; - \u0026#34;--certificatesresolvers.lets-encrypt-ssl.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory\u0026#34; - \u0026#34;--certificatesResolvers.lets-encrypt-ssl.acme.email=YOUR-EMAIL@website.com\u0026#34; networks: monitoring: driver: bridge volumes: influxdb-data: external: false grafana-data: external: false traefik-data: external: false There\u0026rsquo;s quite a lot going on here! I\u0026rsquo;ll try to break it down bit by bit.\nNotice that the ports for the Grafana and InfluxDB containers have been moved to the Traefik container. This is because Traefik will now act as an entry point for our Docker compose application, and will route traffic to other containers.\nTraefik labels We create some labels in the InfluxDB container for our Traefik configuration:\n  - \u0026quot;traefik.http.routers.influxdb-ssl.entryPoints=influxdb-port\u0026quot;: set the container\u0026rsquo;s \u0026ldquo;entrypoint\u0026rdquo; using a Traefik router. It corresponds to the appropriate label in the Traefik container - here I called it influxdb-port. The fourth part of the label can have any arbitrary name (here I chose influxdb-ssl).\n  - \u0026quot;traefik.http.routers.influxdb-ssl.rule=host(`YOUR_DOMAIN`)\u0026quot;: the start of the TLS configuration - fill in the domain of where this application will be hosted. If you\u0026rsquo;re testing locally, it can be set to localhost or variations of it (e.g. monitoring.docker.localhost).\n  - \u0026quot;traefik.http.routers.influxdb-ssl.tls=true\u0026quot;: flag for enabling TLS for this container. Changing this to false can be useful when you\u0026rsquo;re testing the deployment locally, and want to send data to InfluxDB from applications that can\u0026rsquo;t be set to ignore TLS certificates. This is because TLS certificates can\u0026rsquo;t be issued for localhost domains by Traefik as they don\u0026rsquo;t end with a valid top-level domain.\n  - \u0026quot;traefik.http.routers.influxdb-ssl.tls.certResolver=lets-encrypt-ssl\u0026quot;: the name of the certificate resolver - make sure it matches the name of the certificate resolver set in the Traefik container.\n  - \u0026quot;traefik.http.routers.influxdb-ssl.service=influxdb-ssl\u0026quot;: set the name of the Traefik service for this container. This will be the name that the other labels in this container have been given.\n  - \u0026quot;traefik.http.services.influxdb-ssl.loadBalancer.server.port=8086\u0026quot;: sets any ports that Traefik should route traffic to for this container. This is done using a Traefik service, and is effectively a way to tell Traefik how to reach the service. This will be specified by the container\u0026rsquo;s Docker image, so be sure to check which port to use for any other images.\n  Traefik with other containers The Grafana TLS configuration is largely the same, with the appropriate port numbers and router names changed.\nSince we want Grafana to show up when the domain is visited, the following labels handle HTTP to HTTPS redirection using Traefik:\n- \u0026#34;traefik.http.routers.grafana.entryPoints=port80\u0026#34; - \u0026#34;traefik.http.routers.grafana.rule=host(`YOUR_DOMAIN`)\u0026#34; - \u0026#34;traefik.http.routers.grafana.middlewares=grafana-redirect\u0026#34; - \u0026#34;traefik.http.middlewares.grafana-redirect.redirectScheme.scheme=https\u0026#34; - \u0026#34;traefik.http.middlewares.grafana-redirect.redirectScheme.permanent=true\u0026#34; Here we set up Traefik middlewares which are a means to tweak requests (e.g. redirect from one port to another) before it gets passed to the Traefik service. The configuration is fairly straightforward.\nTraefik container configuration Finally, we set up the actual Traefik container:\ntraefik: container_name: traefik image: traefik:v2.3.4 volumes: - traefik-data:/letsencrypt - /var/run/docker.sock:/var/run/docker.sock networks: - monitoring ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; - \u0026#34;8086:8086\u0026#34; First, we set up a persistent volume for the /letencrypt directory which will store any TLS certificates that Traefik obtains from Let\u0026rsquo;s Encrypt. This is important so that certificates don\u0026rsquo;t have to be reissued when the container is restarted.\nNext, we need to mount the Docker socket into the container so that Traefik can get Docker\u0026rsquo;s dynamic configuration.\nWe also open all the ports whose traffic we want to route through Traefik.\ncommand: - \u0026#34;--providers.docker=true\u0026#34; - \u0026#34;--entryPoints.port443.address=:443\u0026#34; - \u0026#34;--entryPoints.port80.address=:80\u0026#34; - \u0026#34;--entryPoints.influxdb-port.address=:8086\u0026#34; We set the providers.docker command to true since we\u0026rsquo;re using Docker. We also set up some Traefik entrypoints to open connections for incoming requests, which correspond to the ports we\u0026rsquo;ve opened in the container.\n- \u0026#34;--certificatesResolvers.lets-encrypt-ssl.acme.tlsChallenge=true\u0026#34; - \u0026#34;--certificatesResolvers.lets-encrypt-ssl.acme.storage=/letsencrypt/acme.json\u0026#34; - \u0026#34;--certificatesresolvers.lets-encrypt-ssl.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory\u0026#34; - \u0026#34;--certificatesResolvers.lets-encrypt-ssl.acme.email=YOUR-EMAIL@website.com\u0026#34; Finally we set up the certificate resolver that will renew and issue our TLS certificates. Note that the second part of these labels can be set to any arbitrary name, and isn\u0026rsquo;t used anywhere.\nThe caServer label determines which Certificate Authority to request TLS certificates from. Since we\u0026rsquo;re getting them from Let\u0026rsquo;s Encrypt, this can either be set to Let\u0026rsquo;s Encrypt\u0026rsquo;s staging (https://acme-staging-v02.api.letsencrypt.org/directory) server or production (https://acme-v02.api.letsencrypt.org/directory) server. There is a limit of 5 certificates per week from Let\u0026rsquo;s Encrypt\u0026rsquo;s production server, so it may be a good idea to use the staging server for testing. For more info on the Let\u0026rsquo;s Encrypt staging environment and Traefik, check the note under this Traefik docs page.\nThe email label is necessary so that Let\u0026rsquo;s Encrypt can contact you about expiring certificates and any issues related to your account.\nThat\u0026rsquo;s it! Your Docker containers are now secured, and traffic that will be sent to and from them will be encrypted.\nFurther improvements There are a couple of small improvements that can be made to the Docker application at this point. Many of the improvements have been implemented in the mentioned GitHub repository mentioned at the start of this post.\n  volumes configurations can be expanded to their verbose form for better readability. This can help with understanding what kind of mount is used, as it\u0026rsquo;s not immediately obvious with the short syntax.\n  An .env file can be added alongside the docker-compose.yml file to store some common variables which are used throughout the docker-compose.yml, to avoid repetition.\n  restart: always can be added to the Docker containers so that they\u0026rsquo;re restarted on boot.\n  Grafana can automatically set up data sources specified under grafana/provisioning/datasources. Other aspects can also be provisioned in a similar manner, including plugins, dashboards, and alert notification channels. For more info, see Grafana provisoning.\n  InfluxDB will run shell scripts located in the docker-entrypoint-initdb.d directory on startup. InfluxQL .iql files can also be included there, but I wouldn\u0026rsquo;t recommend it as they\u0026rsquo;re much less flexible than shell scripts and can lead to non-obvious issues.\n  ","permalink":"https://dominikrys.com/posts/secure-monitoring-solution-docker/","summary":"Motivation During my last internship, I\u0026rsquo;ve been tasked with designing and deploying infrastructure for monitoring a cluster of machines that were used for performance testing. I wrote a blog post detailing high-level choices about it which you can check out here. The post also includes justifications for why I chose to deploy everything in Docker, and why I chose to work with Grafana and InfluxDB as the front-end and time-series database, respectively.","title":"Setting up a TLS-Secured Monitoring Solution in Docker using InfluxDB, Grafana and Traefik"},{"content":" This post is also hosted on the Corda Blog. The main goal behind this post was to provide an easily accessible high-level overview on monitoring Corda nodes. It also showcases part of what I\u0026rsquo;ve done during my summer internship at R3.\n Intro Here at R3, we have a cluster of Corda nodes that we use for performance testing. We have developed a performance testing suite that enables us to establish baseline numbers, quantify improvements from new features, and identify regressions.\nWe recently decided to invest some effort in improving the observability of the overall system so that we could identify regressions and analyse their root cause more efficiently and with less manual work. There is a wealth of metrics exposed by Corda nodes via JMX that can be inspected using tools such as Hawtio (as described in the Corda docs). However, this approach requires plenty of manual intervention and the prerequisite of the test actively running during inspection times.\n Corda JMX metrics visible in Hawtio\n  We had to set up a monitoring infrastructure that would allow us to:\n  Collect the metrics exposed by our Corda nodes via JMX.\n  Collect the metrics not exposed via JMX, such as disk IO and network activity.\n  Store the collected metrics in a centralised database.\n  Visualise, filter, and compare metrics from different time windows using a front end accessible from a web browser.\n  Monitoring is a core aspect of operating Corda nodes efficiently. Therefore, in this post we give you a quick overview of the technologies available and their trade-offs. We also walk you through the capabilities and the architecture of our solution. The described work has been performed on Corda 4.5, but the high-level architecture is really version-agnostic. We hope this can help those of you getting started with monitoring!\nHosting the monitoring infrastructure The first step was to decide how to host the monitoring infrastructure, as that would greatly impact the choice of other tools that we could use. Ultimately it came down to either using a third-party managed service or deploying the infrastructure ourselves in the public cloud.\nThird-party managed services This would be a great option if we wanted a scalable solution without us having to spend too much time setting up the infrastructure and managing it, or if it was to support production workloads that would have strict requirements for high availability. Given that we only wanted to monitor the nodes in our cluster and didn‚Äôt intend for it to scale beyond that, using a third-party service wouldn‚Äôt have been cost-effective as it would have provided many more features than were necessary for us.\nWe also wanted the ability to have multiple users access dashboards simultaneously, and the possibility for user authentication. These features were only accessible in the higher tiers of such services, which provide much more storage and bandwidth than we required.\nSelf-hosting We ended up going down the path of hosting the monitoring infrastructure ourselves in Microsoft Azure. The most popular tools for setting up monitoring solutions provide open source offerings so this was a viable option. It also has the added benefits of giving us full control over the infrastructure and knowing the exact running costs.\nComparison of tools As we chose to self-host the monitoring infrastructure, we had the liberty of choosing from the multitude of open source tools available to set up our infrastructure. We carefully considered which tools to use so that the monitoring infrastructure wouldn‚Äôt require much effort to maintain in the long run.\nEffectively the monitoring infrastructure can be split up into three parts:\n  A metric collection agent.\n  A time-series database (TSDB).\n  A front end for visualising and querying the TSDB.\n  The descriptions of each part below provide details about the tool we chose to use, followed by alternatives that we had also considered.\nMetric collection agent  ‚≠ê Telegraf‚Äî a lightweight open source server agent that can collect and write metrics to and from different sources. It‚Äôs plug-in driven so we could easily set it up using the provided jolokia2 plug-in to collect the metrics exposed through JMX from our Corda nodes.\nTelegraf also provides plug-ins that allow for monitoring various system metrics which aren‚Äôt available through JMX, such as disk IO and networking usage. In our case, it was so convenient to configure that we also deployed it on our Corda node databases with the relevant PostgreSQL and SQL Server plug-ins.\nOther options ‚Äî we also considered Prometheus JMX exporter, jmx2graphite, and jmxtrans. The issue with these is that they are all limited to which metrics they can record and where they can send them to. Telegraf can provide essentially the same functionality as those tools and allows for extensibility, while greatly reducing the number of tools required for maintenance.\nTime-series database  ‚≠ê InfluxDB ‚Äî an open source TSDB from InfluxData, who also develop Telegraf. It‚Äôs easy to install on many different platforms and can be interacted with using the SQL-like InfluxQL query language.\nThis is the TSDB we ended up using. So far it‚Äôs been working well, but a slight gripe with it is that the default query language (InfluxQL) is not quite powerful enough for certain tasks, such as performing calculations on data over different time windows. This is remedied by using InfluxData‚Äôs new Flux query language, albeit at the cost of convenience due to a lack of simplified GUI for it in TSDB front ends ‚Äî the queries have to be written in plain text. Before choosing InfluxDB we‚Äôd recommend checking this aggregate GitHub issue to see if you‚Äôd heavily rely on any query operators that have not yet been implemented in InfluxQL.\nOverall, Flux is still significantly more powerful than Prometheus‚Äô and Graphite‚Äôs query languages. InfluxDB can be the best option if you don‚Äôt mind sacrificing some ease of use sometimes for the ability to write (almost) any query imaginable.\n Prometheus ‚Äî another popular open source TSDB, which is entirely community-driven. It uses a similar data compression algorithm to InfluxDB. The query language (PromQL) is more robust than InfluxQL so you can do more out of the box, although it doesn‚Äôt resemble any particular language so requires learning from scratch.\nOne of the biggest differences in Prometheus compared to other TSDBs is that it pulls instead of pushing metrics. This has some advantages, but also requires additional setting-up on the machines that run your Corda nodes to allow Prometheus to pick up the exported metrics, which entails opening extra ports and setting up firewall rules.\nIt was tough choosing between Prometheus and InfluxDB, but ultimately we went with InfluxDB due to it being maintained by the same people as Telegraf, the potential to write complex queries using Flux and requiring less setup to collect metrics from our Corda nodes.\n Graphite ‚Äî the grandaddy of modern TSDBs. Graphite has been around for longer than Prometheus and InfluxDB, so it‚Äôs a mature and tested tool. The query language resembles some programming languages so it‚Äôs easy to pick up, and it can do more than InfluxQL and PromQL thanks to the huge selection of functions that have been developed over the years.\nBeing the oldest of the bunch has its disadvantages though, most notably that the performance is lacklustre compared to InfluxDB which would have to be accounted for by using a more powerful host VM. Installing Graphite can also be a pain due to its many dependencies if it‚Äôs not deployed in Docker (inspiring projects such as Synthesize that are meant to make the process easier).\nFront end for visualising and querying the TSDB  Example Grafana Dashboard\n  ‚≠ê Grafana ‚Äî an open source tool for interactively visualising data from various data sources. Grafana is by far the most popular tool for interacting with TSDBs with over 1200 contributors on GitHub and a very active community forum. There are many plug-ins available for it, it integrates well with many other services for alerting and authentication, and it makes creating aesthetically pleasing dashboards a breeze.\n Example Chronograf Dashboard\n  Chronograf ‚Äî an open source tool for interacting and visualising data from InfluxDB. Chronograf is very well suited for setups where other products from InfluxData are used, as it‚Äôs better integrated with them compared to Grafana. This could have been a great option if we also used Kapacitor as a real-time streaming data processing engine from InfluxData, to complete their ‚ÄúTICK‚Äù stack.\nGiven that Chronograf has fewer features than Grafana and is significantly less popular, which can make getting support for it more difficult, we went with Grafana.\nExtra considerations Why deploy in Docker?  As all tools we chose provided official Docker images, we decided to deploy our monitoring infrastructure as a Docker Compose application in an Azure VM. This has many benefits:\n  Deploying the monitoring infrastructure is a one-step process and is easily reproducible.\n  Updating and downgrading individual services is straightforward ‚Äî just adjust the version of the Docker images!\n  It‚Äôs easy to manage each container‚Äôs data as it‚Äôs kept in separate Docker volumes.\n  The solution can be easily tested locally and will behave the same locally as on a production server.\n  Securing the monitoring infrastructure  To encrypt the traffic coming in and out of our monitoring infrastructure, we used Traefik which can automatically renew and obtain TLS certificates for our monitoring infrastructure from Let‚Äôs Encrypt. Traefik is an open source edge router that acts as a reverse proxy for our Docker containers. It can be deployed using official Docker images, so it integrates perfectly into our Docker Compose application.\nWe defined separate Traefik services and routers for Grafana and InfluxDB that take care of appropriate routing, HTTP/HTTPS redirection, and TLS configuration. This was all done in a declarative way using labels in our Docker compose file.\nGrafana supports user authentication, which can be integrated with many different services including Azure and GitHub. This also allows for easy management of permissions of the users accessing our Grafana dashboards.\nDeployment of Telegraf Telegraf can be installed on the machines running Corda in a variety of ways and works as a stand-alone tool. Setting it up is relatively straightforward, so you can choose whichever installation method most suits your setup.\nComplete infrastructure architecture The complete architecture of our infrastructure looks as follows:\n TLS is terminated at our reverse proxy (Traefik). This means that traffic between the proxy and Grafana/InfluxDB is not encrypted, but this isn‚Äôt an issue since all these services are running in a single secured machine.\nFinal result We set up a Grafana dashboard with metrics for each Corda node in our cluster. The dashboard features high-level flow metrics first, followed by internal operation metrics (P2P, caches), and finally system-level metrics (JVM, disk IO, network). A part of this dashboard is shown below.\n  We also have a dashboard with a summary of the results from our performance testing suite, which helps us inspect the results quickly and identify potential regressions. A part of it that shows throughput numbers for some of our test cases is shown below. This is made possible by sending data from our test suite running JMeter to InfluxDB using the JMeter InfluxDB Backend Listener.\n The ‚ÄúDifference‚Äù column displays the results of a Flux query performed on our InfluxDB database, repeated for every test case using Grafana‚Äôs variable feature. It calculates the difference in results of each test case between the currently specified run and a run that happened a certain time ago (in this example 24 hours ago).\nConclusion A complete solution for monitoring Corda nodes can be set up entirely using open source tools without compromises. Many of the tools can be mixed and matched, so it‚Äôs possible to adjust the foundation described in this post to better fit your individual needs and existing setup.\nOne of the most impactful choices to be made when setting up a monitoring solution for Corda nodes is the choice of TSDB, as that will greatly affect the performance and usability of your dashboards. InfluxDB and Prometheus are strong TSDB options which have many discerning features that can make one more favourable over the other, depending on your requirements.\nFor more information on monitoring Corda nodes, check the following articles:\n  Monitoring Corda Nodes With Prometheus, Grafana and ELK on Docker\n  Monitoring Corda Nodes using Prometheus and Grafana\n  Monitoring Corda Nodes (Part 1)\n  The Corda documentation is also an amazing resource:\n  Node metrics\n  Node monitoring and logging\n  Want to learn more about building awesome blockchain applications on Corda? Be sure to visit corda.net, check out our community page to learn how to connect with other Corda developers, and sign up for one of our newsletters for the latest updates.\n‚Äî Dominik Rys is a Software Engineer Intern at R3, an enterprise blockchain software firm working with a global ecosystem of more than 350 participants across multiple industries from both the private and public sectors to develop on Corda, its open source blockchain platform, and Corda Enterprise, a commercial version of Corda for enterprise usage.\n","permalink":"https://dominikrys.com/posts/monitoring-corda-nodes/","summary":"This post is also hosted on the Corda Blog. The main goal behind this post was to provide an easily accessible high-level overview on monitoring Corda nodes. It also showcases part of what I\u0026rsquo;ve done during my summer internship at R3.\n Intro Here at R3, we have a cluster of Corda nodes that we use for performance testing. We have developed a performance testing suite that enables us to establish baseline numbers, quantify improvements from new features, and identify regressions.","title":"Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf"},{"content":"A couple of months ago I wrote a CHIP-8 emulator in C++17, as I wanted to learn about emulation and expand my C++ knowledge outside of work. In this post I\u0026rsquo;ll explain how I went about compiling the emulator which was designed to run natively, to also run on the web using the magic of WebAssembly. You can try out the result here.\nMy main motivation for getting the emulator working on the web was that in its current state, it took some effort to get it up and running. I could send someone the pre-compiled binary or give building instructions, but those aren\u0026rsquo;t guaranteed to work on every platform. Ideally, I wanted a solution that can be hosted on the web, and I recently heard about this cool new \u0026ldquo;WebAssembly\u0026rdquo; thing that seemed like the perfect solution.\nWebAssembly is a binary instruction format that runs on modern web browsers and allows apps to run at \u0026ldquo;near-native speed\u0026rdquo;. In reality, there is a performance hit of about 50% relative to their native counterparts so it won\u0026rsquo;t be great at running AI or HFT algorithms in your web browser, but it will be good enough to play Space Invaders.\nTo get our Wasm output, Emscripten can be used. It\u0026rsquo;s a toolchain that can compile C, C++, and any language which uses LLVM into WebAssembly.\nArmed with these tools, the idea was simple: compile my emulator using Emscripten, sort out any errors, and deploy it on a website. An afternoon\u0026rsquo;s work, right? \u0026hellip;not quite. Turns out that there were a couple of pitfalls along the way, so I thought I\u0026rsquo;d document my journey of compiling a loop-based C++ program running into WebAssembly for future me\u0026rsquo;s reference once WebAssembly takes over the world, or for anyone else that may be attempting a similar task themselves\nSetting up Emscripten I\u0026rsquo;ve set up Emscripten and did all the development described in this post on a Windows 10 machine (Unix fanboys, please stay with me - mentions of Windows end here). The setup will look very similar on other platforms and a lot of the code/configuration I mention should work on Mac and Linux, and if not then with minimal adjustments.\nMozilla\u0026rsquo;s article on compiling C/C++ modules to WebAssembly was a great crash-course for setting up and starting to work with Emscripten. I recommend it as a starting point for anything WebAssembly related.\nIt\u0026rsquo;s worth pointing out there aren\u0026rsquo;t many IDE plugins or integrations for Emscripten as of writing this post. I tried to hack as much integration as I could into CLion, but what I managed to get to work caused issues for me later on. Emscripten relies on its own compilers and linkers a lot, so I would recommend sticking solely to the terminal to save yourself some headaches.\nCMake and Emscripten Since my emulator is a not a single file as in the Mozilla examples mentioned in the section above, the most appropriate way to compile the emulator was to use CMake. I took the compiler flags mentioned in the Mozilla examples and added them to my CMakeLists.txt:\nset(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} -s WASM=1 --shell-file ${CMAKE_CURRENT_LIST_DIR}/web/shell_minimal.html\u0026#34;)Next I created a sub-directory in my root project directory for my CMake output (called cmake-build-emscripten) and called the following from it:\n  To generate the project build system:\nemcmake cmake -G \u0026#34;CodeBlocks - MinGW Makefiles\u0026#34; ..   To compile the code:\nmingw32-make   Potential CMake problem If you\u0026rsquo;re using C++17 features, you may get No such file or directory errors referencing certain headers. This can be caused by accidentally compiling the code outside of Emscripten either by yourself or by your IDE. The sure-fire way to fix these is to clear the CMake project build system directory and re-create it.\nAdding Emscripten sections to CMakeLists I wanted to maintain the ability to compile my code natively as well as being able to compile it using Emscripten. The CMake EMSCRIPTEN variable solves this problem, as it\u0026rsquo;s set to true when compiling with Emscripten. Note that when compiling with Emscripten, the CMake UNIX variable will also be true, so make sure your branching logic is correct.\nFor the full CMakeLists.txt that I ended up using, look here.\nInitial compile I was almost ready to compile the code. Since the emulator is using SDL2 for audio and graphics, I also needed to specify the -s USE_SDL=2 compiler flag. When this is specified, Emscripten will automatically download the SDL2 Emscripten port.\nI hardcoded a ROM to the path, and compiled the source otherwise unaltered:\nEmscripten Release -- Configuring done -- Generating done -- Build files have been written to: C:/dev/git/chip8/cmake-build-emscripten Scanning dependencies of target chip8 [ 14%] Building CXX object CMakeFiles/chip8.dir/src/Chip8.cpp.o [ 28%] Building CXX object CMakeFiles/chip8.dir/src/Renderer.cpp.o [ 42%] Building CXX object CMakeFiles/chip8.dir/src/KeyboardHandler.cpp.o [ 57%] Building CXX object CMakeFiles/chip8.dir/src/Configurator.cpp.o [ 71%] Building CXX object CMakeFiles/chip8.dir/src/Audio.cpp.o [ 85%] Building CXX object CMakeFiles/chip8.dir/src/Main.cpp.o [100%] Linking CXX executable chip8.js [100%] Built target chip8 It\u0026hellip; worked? I hosted the generated page locally with Python\npython3 -m http.server And had a look at it in Chrome. I found that the screen was blank, but there was an exception in the JavaScript console:\n Time to attempt some debugging!\nDebugging Emscripten Investigating the JavaScript exception isn\u0026rsquo;t particularly useful, as at that point the code has been generated by Emscripten and is very different to the source code. There is a section on debugging in the Emscripten docs, but in essence the debugging tools aren\u0026rsquo;t currently particularly robust and will require modifying the program\u0026rsquo;s original source code.\nThe most helpful ways to debug Emscripten code that I found are:\n  Setting the ASSERTIONS=2 compiler flag. This catches some potential issues, but isn\u0026rsquo;t particularly useful by itself. I also found this flag to crop up with red herrings sometimes which got me debugging issues that were either fine to ignore, or went away by themselves as I developed more of the code.\n  Handling C++ exceptions from JavaScript. This requires some extra compiler and linker arguments and some extra code, but will give you readable exceptions in the JavaScript console. Very useful if you know which function is throwing exceptions.\n  Manual print statements. Not particularly sophisticated, but works well enough and helped me debug this first exception. Note that you should flush using std::endl or \u0026quot;\\n\u0026quot; when using std::cout statements or else your messages won\u0026rsquo;t get printed while your program is running, and only when it terminates.\n  File system access in Emscripten The reason for the exception turned out to be simple - it couldn\u0026rsquo;t find the ROM I specified to load. The reason for this is that WebAssembly is designed to be secure and to run in a sandboxed execution environment - the file that I was trying to load not in the Wasm sandbox.\nTo use files within Emscripten, a File System API is provided as well as a way to package files. Since I already had the C++ code for reading files, I went down the packaging files route. There are two compiler options that can do this job:\n  --preload-file \u0026lt;name\u0026gt;: allows you to pre-load a file or directory before running the compiled code asynchronously. The result is a .data file alongside your generated .html and .js which contains the preloaded files. This is the option I went with.\n  --embed-file \u0026lt;file\u0026gt;: allows you to embed a file or path into the generated script. The result is that the files will be embedded inside your generated .js file. This is less efficient than pre-loading and should only be used when there are few files to load.\n  After packaging the files, they can be accessed with file API calls from your C/C++ code as if the files existed locally.\nOnce I packaged the files properly, everything compiled again. I checked the website and there weren\u0026rsquo;t any exceptions there, but the output was blank and the page was frozen:\n I also got the following warning:\nThe AudioContext was not allowed to start. It must be resumed (or created) after a user gesture on the page. https://goo.gl/7K7WLu For the time being, I decided to get rid of the audio handling code and sort this out later.\nEmscripten loops To get the screen buffer to update and not freeze the tab, I had to change the current loop that my current emulator runs on into an Emscripten loop. This is because the web browser event model uses co-operative multitasking, so each event gets a turn to run and has to return control to the browser. My code was blocking and never gave control back to the browser, so the tab froze and the display didn\u0026rsquo;t update.\nAfter a quick Google looking for some real-world examples on how to write an Emscripten loop, I found James MacKenzie\u0026rsquo;s blog which has been a huge help on getting this to work.\nA traditional loop in a C++ program using SDL looks like the following:\nwhile(running) { renderFrame(); SDL_Delay(timeToNextFrame()); } And once rewritten to use Emscripten loops look like this:\nemscripten_set_main_loop(renderFrame, 0, 1); Where the full signature of emscripten_set_main_loop is:\nemscripten_set_main_loop(em_callback_func func, int fps, int simulate_infinite_loop) emscripten_set_main_loop() simulates an infinite loop, but in reality just calls the loop function a specified number of times a second. The amount of times that this loop gets called a second is specified by the second argument, however the Emscripten docs mention that it\u0026rsquo;s \u0026ldquo;HIGHLY recommended\u0026rdquo; to set this to 0 or a negative value when doing any rendering. The site will then use the browser‚Äôs requestAnimationFrame method to call the main loop function which we will revisit later.\nRewriting for a global function in the main loop There is a major side effect of having to call a global function in my main loop - every object called within that global function also has to be accessible globally, or be local to that function.\nMy code wasn\u0026rsquo;t designed with this in mind and would have been structured differently if I was writing it for Emscripten from the start. Anyway, I had to make a couple of objects global in my small Main.cpp file to make it accessible from the main loop. It\u0026rsquo;s not a particularly elegant solution, but for this relatively simple project it sufficed.\nAs an effect of writing code this way your IDE may also complain about not being able to catch exceptions from variables with static storage duration - this is not a problem, and Emscripten will still be able to catch them for us if we add the exception handling in Javasript code that was described in the debugging section.\nMy code went from looking something like this:\n// -- Headers --  int main() { try { Config config{}; Chip8 chip8{config.mode_}; chip8.loadRom(config.romPath_); KeyboardHandler keyboardHandler(chip8.keys()); Renderer renderer{\u0026#34;CHIP-8 Emulator\u0026#34;, VIDEO_WIDTH, VIDEO_HEIGHT, config.videoScale_}; const double cycleDelay = (1.0 / config.cpuFrequency_) * 1000000000; Timer cycleTimer(cycleDelay); bool quit = false; while (!quit) { quit = keyboardHandler.handle(); if (cycleTimer.intervalElapsed()) { chip8.cycle(); if (chip8.drawFlag()) { // -- Graphics rendering code --  } } } } catch (const std::exception \u0026amp;e) { std::cerr \u0026lt;\u0026lt; e.what(); std::exit(EXIT_FAILURE); } return EXIT_SUCCESS; } To this:\n// -- Headers --  Config config{}; Chip8 chip8{config.mode_}; KeyboardHandler keyboardHandler(chip8.keys()); Renderer renderer{\u0026#34;WASM CHIP-8 Emulator\u0026#34;, VIDEO_WIDTH, VIDEO_HEIGHT, 13}; int cyclesPerFrame = 10; void mainLoop() { keyboardHandler.handle(); chip8.cycle(); if (chip8.drawFlag()) { // -- Graphics rendering code --  } } int main() { emscripten_set_main_loop(mainLoop, 0, 0); return EXIT_SUCCESS; } At this point I\u0026rsquo;ve also added a separate Main.cpp file just for Emscripten and added rules to pick up the right one in CMakeLists.txt depending on the compiler. The code has diverged a lot from my original Main.cpp, and adding preprocessor made the code difficult to trace through.\nEmterpreter Due to the amount of effort that may be required to rewrite traditional loops into Emscripten loops, depending on your project, Emscripten also provides Emterpreter. This allow you to keep traditional loops by adding emscripten_sleep() calls to your code:\nwhile(running) { renderFrame(); emscripten_sleep(timeToNextFrame()); } I gave this a go myself before rewriting my code for emscripten_set_main_loop() as I thought this could be a nice way to get things working quickly, but I found it to be a cumbersome process for a couple of reasons:\n  Emterpreter is not available on the default LLVM backend that Emscripten uses. It\u0026rsquo;s necessary to switch to the fastcomp backend, which is considered a legacy backend by Emscripten.\n  When compiling the code, you get various messages mentioning that code compiled using Emterpreter may run slowly.\n  Emterpreter seems to only be recommended to be used when absolutely necessary by the Emscripten devs, and even then only by certain parts of your program.\n  It simply didn\u0026rsquo;t work for my project. I got various errors and decided to switch back and to it the \u0026ldquo;proper\u0026rdquo; way, as also more support would be available for doing things that way.\n  Frame rate issues I compiled the code after rewriting using an Emscripten loop and I got the emulator working!\n The emulator was very slow, however. I found that this is to do with the requestAnimationFrame method which I mentioned previously that is used to call the Emscripten main loop function.\nThe Mozilla docs state that requestAnimationFrame gets called \u0026ldquo;usually 60 times per second, but will generally match the display refresh rate in most web browsers\u0026rdquo;. This was a problem, as my main loop simulated one CHIP-8 cycle every time it was called. Effectively this meant that my the emulator ran at a frequency of 60Hz when compiled with Emscripten, where most CHIP-8 ROMs run well at 1000-1500Hz depending on the game (this isn\u0026rsquo;t something that can be determined on the fly, as in ordinary game loops).\nTo fix this issue, I added a constant which determines how many cycles to emulate every time the main Emscripten loop is given control. Calculating this constant wasn\u0026rsquo;t very straightforward, as the frame rate which ran natively didn\u0026rsquo;t directly translate to what ran in Emscripten. For example, if I ran a game at 1000Hz natively, I should be able to divide that by 60 to get the amount of cycles to emulate between every frame in an Emscripten loop - around 17 in this case. This wasn\u0026rsquo;t the case however, as the Emscripten loop ran much faster than anticipated: to get an Emscripten loop running at a similar speed to one running natively at 1000Hz, I set the amount of loops to emulate per frame to 10. I found this through a process of trial-and-error as there didn\u0026rsquo;t seem to be a good way to calculate this difference upfront.\nAlso it\u0026rsquo;s worth nothing that not every screen will refresh at 60 frames a second, which is something to consider if you want the program to run at the same speed for everyone.\nThe logic for emulating a set amount of cycles per frame looked as follows:\nint cyclesPerFrame = 10; void mainLoop() { for (int i = 0; i \u0026lt; cyclesPerFrame; i++) { chip8.cycle(); } } int main() { emscripten_set_main_loop(mainLoop, 0, 0); } After compiling the emulator again with this change, everything worked as expected. Surprisingly, the keyboard also worked perfectly and didn\u0026rsquo;t require any intervention.\nExporting C++ functions to call from JavaScript To finish this off, I wanted to add a dropdown so it\u0026rsquo;s possible to select a game to load (turns out Pong gets a bit boring after a while). First, I added a function to my C++ code which will get exported so it can be called from JavaScript. There is a page in the Emscripted docs which explains this in good detail. The function looked as follows:\nextern \u0026#34;C\u0026#34; { void loadRom(char *path, int cyclesPerFrame_) { cyclesPerFrame = cyclesPerFrame_; chip8.reset(); chip8.loadRom(path); } } The function is wrapped in extern \u0026quot;C\u0026quot; to prevent C++ name mangling. I also added a cyclesPerFrame_ parameter as many CHIP-8 ROMs need to have their frame rate adjusted to run at an appropriate speed.\nNext, I exported the function to enable it to be called from JavaScript. In order to call a C++ function from JavaScript, I also need to export the runtime ccall and/or cwrap methods which are called on the Module JavaScript object which the Emscripten-generated code calls at various points in its execution. To do this, I added the following to my compiler flags:\n-s EXPORTED_FUNCTIONS=\\\u0026#34;[\u0026#39;_main\u0026#39;, \u0026#39;_loadRom\u0026#39;]\\\u0026#34; \\ -s EXPORTED_RUNTIME_METHODS=\\\u0026#34;[\u0026#39;ccall\u0026#39;, \u0026#39;cwrap\u0026#39;]\\\u0026#34; \\ -s ALLOW_MEMORY_GROWTH=1 \\ --no-heap-copy \\ Note that I also exported main as I want to be able to call the main function myself and not start it automatically when the page is loaded. The -s ALLOW_MEMORY_GROWTH=1 and --no-heap-copy flags are necessary as the memory used by the WASM code will increase when we load a game. This shouldn\u0026rsquo;t have any overhead when compiling to WebAssembly.\nNext, I made some changes to the shell.html file:\n  I set noInitialRun to true in the JavaScript Module object so it doesn\u0026rsquo;t automatically run the emulator when the page loads.\n  Added a dropdown for selecting which ROM to load. Each option includes a path to a ROM to load and specifies the amount of cycles to emulate per frame, which differs per game.\n\u0026lt;div class=\u0026#34;emscripten\u0026#34; id=\u0026#34;menu\u0026#34;\u0026gt; \u0026lt;select id=\u0026#34;rom-dropdown\u0026#34;\u0026gt; \u0026lt;option value=\u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;games/Pong (1 player).ch8\u0026#34;,\u0026#34;cyclesPerFrame\u0026#34;:10}\u0026#39;\u0026gt; PONG \u0026lt;/option\u0026gt; \u0026lt;!-- More ROMs --\u0026gt; \u0026lt;/select\u0026gt; \u0026lt;/div\u0026gt;   Added a JavaScript listener to check for changes in the dropdown when the WebAssembly code has loaded, and call the exported C++ function:\nfunction getRomOptionsFromDropdown(optionText) { let romOptions = JSON.parse(optionText); const romName = romOptions[\u0026#34;name\u0026#34;]; const romPath = \u0026#34;bin/roms/revival/\u0026#34; + romName + \u0026#34;\\0\u0026#34;; selectedRomUInt8Array = new TextEncoder().encode(romPath); cyclesPerFrame = romOptions[\u0026#34;cyclesPerFrame\u0026#34;]; Module.ccall( \u0026#34;loadRom\u0026#34;, \u0026#34;null\u0026#34;, [\u0026#34;array\u0026#34;, \u0026#34;number\u0026#34;], [selectedRomUInt8Array, cyclesPerFrame] ); } Module[\u0026#34;onRuntimeInitialized\u0026#34;] = function () { getRomOptionsFromDropdown(document.querySelector(\u0026#34;#rom-dropdown\u0026#34;).value); document.querySelector(\u0026#34;#rom-dropdown\u0026#34;).onchange = function (event) { getRomOptionsFromDropdown(event.target.value); };  Note that JavaScript strings are not null terminated. Since we\u0026rsquo;re passing the string with the ROM path from JavaScript to C++ code, we have to append a null character manually as C and C++ strings must be null-terminated. If the string is not null-terminated, then certain C++ functions which rely on the string being null-terminated won\u0026rsquo;t work as expected.    Audio I mentioned that I\u0026rsquo;d come back to audio earlier in this post. The only audio in the emulator is a simple \u0026ldquo;beep\u0026rdquo;, implemented as a sine wave that which gets played on a separate thread. To compile the code I had to add some extra compiler flags as described in the Emscripten Pthreads support page to allow for multithreading in Emscripten.\nI compiled the code with the audio enabled and checked how Chrome treats it. The audio played when it should, however it ended up being a high pitched noise of varying frequencies which didn\u0026rsquo;t stop - not exactly what I wanted. Changing various settings didn\u0026rsquo;t seem to have an effect on the noise produced. I also gave it a go in Firefox, which required me to enable a flag as the support in Firefox for Webassembly pthreads is currently experimental. Once the flag was enabled, Firefox had various issues with the audio device and I didn\u0026rsquo;t see pursuing this any further worthwhile.\nIn order to make this work, I a good way would be to handle the audio entirely in JavaScript and query the emulator for when a sound should play. Since I didn\u0026rsquo;t see much value in adding sound to the emulator, I left it out.\nFinishing touches That\u0026rsquo;s pretty much it - I managed to compile the emulator into WebAssembly, I added the ability to play different games and to host the emulator online. To finish the emulator off, I added some CSS styling, a start/stop button and instructions on how to play which was easy to do by editing the default Emscripten shell file.\nAn extra thing which was worth doing is checking how the site behaved in different web browsers. For example I made the assumption in my JavaScript code that the game picker dropdown will always not have a game selected when the page is loaded. This held for Chrome, but not for Firefox which remembers what the last option that the user picked in a dropdown was, so I had to handle that case accordingly.\nThe end! I hope this post was somewhat insightful for anyone looking at compiling their own C or C++ code into WebAssembly. I think there\u0026rsquo;s huge potential in the technology, and can be very useful for computationally expensive tasks which aren\u0026rsquo;t viable to be ran using JavaScript. For examples of more sophisticated projects using WebAssembly and some inspiration, I recommend having a look Made with WebAssembly.\n","permalink":"https://dominikrys.com/posts/compiling-chip8-to-wasm/","summary":"A couple of months ago I wrote a CHIP-8 emulator in C++17, as I wanted to learn about emulation and expand my C++ knowledge outside of work. In this post I\u0026rsquo;ll explain how I went about compiling the emulator which was designed to run natively, to also run on the web using the magic of WebAssembly. You can try out the result here.\nMy main motivation for getting the emulator working on the web was that in its current state, it took some effort to get it up and running.","title":"Compiling a C++ CHIP-8 Emulator to WebAssembly"},{"content":"Hi, I\u0026rsquo;m Dom. Welcome to my first blog post!\nMy main objective for this blog is to start documenting my work more. Having had completed some small side projects and interning at some companies, I wanted a place to document the details of my past work. I thought that a blog is the perfect place for this.\nThe tech stack I used to set up this blog is quite straightforward. Hugo is the static site generator responsible for most of the heavy work, allowing me to write posts simply in Markdown and then rapidly generate the web pages. The site is then hosted on GitHub Pages (with Jekyll bypassed).\nAlternative static site generators I\u0026rsquo;ve also considered are Gatsby and Jekyll:\n  Gatsby is another popular solution, however I didn\u0026rsquo;t see it as being easily maintainable as it may require some tinkering with React. I wanted something that serves only as a blog generator, allowing me to focus on writing blog posts and not tweaking the tools.\n  Jekyll is used by default on GitHub pages so it felt like the more natural choice. I\u0026rsquo;ve tried to set it up in the past, but ran into many issues on the way stemming from Ruby dependencies on both Windows and Mac. Hugo worked out of the box and I found it to be a faster framework\n  I hope you find the content on this blog useful!\n","permalink":"https://dominikrys.com/posts/first-blog-post/","summary":"Hi, I\u0026rsquo;m Dom. Welcome to my first blog post!\nMy main objective for this blog is to start documenting my work more. Having had completed some small side projects and interning at some companies, I wanted a place to document the details of my past work. I thought that a blog is the perfect place for this.\nThe tech stack I used to set up this blog is quite straightforward. Hugo is the static site generator responsible for most of the heavy work, allowing me to write posts simply in Markdown and then rapidly generate the web pages.","title":"First Blog Post"}]