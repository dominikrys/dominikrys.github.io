<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf | Dominik Rys</title>
<meta name=keywords content="Docker,Site Reliability Engineering,InfluxDB,Grafana,Traefik"><meta name=description content="
This post is also hosted on the Corda Blog. The main goal behind this post was to provide an easily accessible high-level overview on monitoring Corda nodes. It also showcases part of what I&rsquo;ve done during my summer internship at R3.

Intro
Here at R3, we have a cluster of Corda nodes that we use for performance testing. We have developed a performance testing suite that enables us to establish baseline numbers, quantify improvements from new features, and identify regressions."><meta name=author content><link rel=canonical href=https://dominikrys.com/posts/monitoring-corda-nodes/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://dominikrys.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dominikrys.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dominikrys.com/favicon-32x32.png><link rel=apple-touch-icon href=https://dominikrys.com/apple-touch-icon.png><link rel=mask-icon href=https://dominikrys.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dominikrys.com/posts/monitoring-corda-nodes/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-FZL8HVTHR8"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-FZL8HVTHR8")}</script><meta property="og:url" content="https://dominikrys.com/posts/monitoring-corda-nodes/"><meta property="og:site_name" content="Dominik Rys"><meta property="og:title" content="Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf"><meta property="og:description" content=" This post is also hosted on the Corda Blog. The main goal behind this post was to provide an easily accessible high-level overview on monitoring Corda nodes. It also showcases part of what I’ve done during my summer internship at R3.
Intro Here at R3, we have a cluster of Corda nodes that we use for performance testing. We have developed a performance testing suite that enables us to establish baseline numbers, quantify improvements from new features, and identify regressions."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-09-21T16:52:48+01:00"><meta property="article:modified_time" content="2020-09-21T16:52:48+01:00"><meta property="article:tag" content="Docker"><meta property="article:tag" content="Site Reliability Engineering"><meta property="article:tag" content="InfluxDB"><meta property="article:tag" content="Grafana"><meta property="article:tag" content="Traefik"><meta property="og:image" content="https://dominikrys.com/img/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dominikrys.com/img/cover.png"><meta name=twitter:title content="Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf"><meta name=twitter:description content="
This post is also hosted on the Corda Blog. The main goal behind this post was to provide an easily accessible high-level overview on monitoring Corda nodes. It also showcases part of what I&rsquo;ve done during my summer internship at R3.

Intro
Here at R3, we have a cluster of Corda nodes that we use for performance testing. We have developed a performance testing suite that enables us to establish baseline numbers, quantify improvements from new features, and identify regressions."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dominikrys.com/posts/"},{"@type":"ListItem","position":2,"name":"Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf","item":"https://dominikrys.com/posts/monitoring-corda-nodes/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf","name":"Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf","description":" This post is also hosted on the Corda Blog. The main goal behind this post was to provide an easily accessible high-level overview on monitoring Corda nodes. It also showcases part of what I\u0026rsquo;ve done during my summer internship at R3.\nIntro Here at R3, we have a cluster of Corda nodes that we use for performance testing. We have developed a performance testing suite that enables us to establish baseline numbers, quantify improvements from new features, and identify regressions.\n","keywords":["Docker","Site Reliability Engineering","InfluxDB","Grafana","Traefik"],"articleBody":" This post is also hosted on the Corda Blog. The main goal behind this post was to provide an easily accessible high-level overview on monitoring Corda nodes. It also showcases part of what I’ve done during my summer internship at R3.\nIntro Here at R3, we have a cluster of Corda nodes that we use for performance testing. We have developed a performance testing suite that enables us to establish baseline numbers, quantify improvements from new features, and identify regressions.\nWe recently decided to invest some effort in improving the observability of the overall system so that we could identify regressions and analyse their root cause more efficiently and with less manual work. There is a wealth of metrics exposed by Corda nodes via JMX that can be inspected using tools such as Hawtio (as described in the Corda docs). However, this approach requires plenty of manual intervention and the prerequisite of the test actively running during inspection times.\nCorda JMX metrics visible in Hawtio\nWe had to set up a monitoring infrastructure that would allow us to:\nCollect the metrics exposed by our Corda nodes via JMX.\nCollect the metrics not exposed via JMX, such as disk IO and network activity.\nStore the collected metrics in a centralised database.\nVisualise, filter, and compare metrics from different time windows using a front end accessible from a web browser.\nMonitoring is a core aspect of operating Corda nodes efficiently. Therefore, in this post we give you a quick overview of the technologies available and their trade-offs. We also walk you through the capabilities and the architecture of our solution. The described work has been performed on Corda 4.5, but the high-level architecture is really version-agnostic. We hope this can help those of you getting started with monitoring!\nHosting the monitoring infrastructure The first step was to decide how to host the monitoring infrastructure, as that would greatly impact the choice of other tools that we could use. Ultimately it came down to either using a third-party managed service or deploying the infrastructure ourselves in the public cloud.\nThird-party managed services This would be a great option if we wanted a scalable solution without us having to spend too much time setting up the infrastructure and managing it, or if it was to support production workloads that would have strict requirements for high availability. Given that we only wanted to monitor the nodes in our cluster and didn’t intend for it to scale beyond that, using a third-party service wouldn’t have been cost-effective as it would have provided many more features than were necessary for us.\nWe also wanted the ability to have multiple users access dashboards simultaneously, and the possibility for user authentication. These features were only accessible in the higher tiers of such services, which provide much more storage and bandwidth than we required.\nSelf-hosting We ended up going down the path of hosting the monitoring infrastructure ourselves in Microsoft Azure. The most popular tools for setting up monitoring solutions provide open source offerings so this was a viable option. It also has the added benefits of giving us full control over the infrastructure and knowing the exact running costs.\nComparison of tools As we chose to self-host the monitoring infrastructure, we had the liberty of choosing from the multitude of open source tools available to set up our infrastructure. We carefully considered which tools to use so that the monitoring infrastructure wouldn’t require much effort to maintain in the long run.\nEffectively the monitoring infrastructure can be split up into three parts:\nA metric collection agent.\nA time-series database (TSDB).\nA front end for visualising and querying the TSDB.\nThe descriptions of each part below provide details about the tool we chose to use, followed by alternatives that we had also considered.\nMetric collection agent ⭐ Telegraf— a lightweight open source server agent that can collect and write metrics to and from different sources. It’s plug-in driven so we could easily set it up using the provided jolokia2 plug-in to collect the metrics exposed through JMX from our Corda nodes.\nTelegraf also provides plug-ins that allow for monitoring various system metrics which aren’t available through JMX, such as disk IO and networking usage. In our case, it was so convenient to configure that we also deployed it on our Corda node databases with the relevant PostgreSQL and SQL Server plug-ins.\nOther options — we also considered Prometheus JMX exporter, jmx2graphite, and jmxtrans. The issue with these is that they are all limited to which metrics they can record and where they can send them to. Telegraf can provide essentially the same functionality as those tools and allows for extensibility, while greatly reducing the number of tools required for maintenance.\nTime-series database ⭐ InfluxDB — an open source TSDB from InfluxData, who also develop Telegraf. It’s easy to install on many different platforms and can be interacted with using the SQL-like InfluxQL query language.\nThis is the TSDB we ended up using. So far it’s been working well, but a slight gripe with it is that the default query language (InfluxQL) is not quite powerful enough for certain tasks, such as performing calculations on data over different time windows. This is remedied by using InfluxData’s new Flux query language, albeit at the cost of convenience due to a lack of simplified GUI for it in TSDB front ends — the queries have to be written in plain text. Before choosing InfluxDB we’d recommend checking this aggregate GitHub issue to see if you’d heavily rely on any query operators that have not yet been implemented in InfluxQL.\nOverall, Flux is still significantly more powerful than Prometheus’ and Graphite’s query languages. InfluxDB can be the best option if you don’t mind sacrificing some ease of use sometimes for the ability to write (almost) any query imaginable.\nPrometheus — another popular open source TSDB, which is entirely community-driven. It uses a similar data compression algorithm to InfluxDB. The query language (PromQL) is more robust than InfluxQL so you can do more out of the box, although it doesn’t resemble any particular language so requires learning from scratch.\nOne of the biggest differences in Prometheus compared to other TSDBs is that it pulls instead of pushing metrics. This has some advantages, but also requires additional setting-up on the machines that run your Corda nodes to allow Prometheus to pick up the exported metrics, which entails opening extra ports and setting up firewall rules.\nIt was tough choosing between Prometheus and InfluxDB, but ultimately we went with InfluxDB due to it being maintained by the same people as Telegraf, the potential to write complex queries using Flux and requiring less setup to collect metrics from our Corda nodes.\nGraphite — the grandaddy of modern TSDBs. Graphite has been around for longer than Prometheus and InfluxDB, so it’s a mature and tested tool. The query language resembles some programming languages so it’s easy to pick up, and it can do more than InfluxQL and PromQL thanks to the huge selection of functions that have been developed over the years.\nBeing the oldest of the bunch has its disadvantages though, most notably that the performance is lacklustre compared to InfluxDB which would have to be accounted for by using a more powerful host VM. Installing Graphite can also be a pain due to its many dependencies if it’s not deployed in Docker (inspiring projects such as Synthesize that are meant to make the process easier).\nFront end for visualising and querying the TSDB Example Grafana Dashboard\n⭐ Grafana — an open source tool for interactively visualising data from various data sources. Grafana is by far the most popular tool for interacting with TSDBs with over 1200 contributors on GitHub and a very active community forum. There are many plug-ins available for it, it integrates well with many other services for alerting and authentication, and it makes creating aesthetically pleasing dashboards a breeze.\nExample Chronograf Dashboard\nChronograf — an open source tool for interacting and visualising data from InfluxDB. Chronograf is very well suited for setups where other products from InfluxData are used, as it’s better integrated with them compared to Grafana. This could have been a great option if we also used Kapacitor as a real-time streaming data processing engine from InfluxData, to complete their “TICK” stack.\nGiven that Chronograf has fewer features than Grafana and is significantly less popular, which can make getting support for it more difficult, we went with Grafana.\nExtra considerations Why deploy in Docker? As all tools we chose provided official Docker images, we decided to deploy our monitoring infrastructure as a Docker Compose application in an Azure VM. This has many benefits:\nDeploying the monitoring infrastructure is a one-step process and is easily reproducible.\nUpdating and downgrading individual services is straightforward — just adjust the version of the Docker images!\nIt’s easy to manage each container’s data as it’s kept in separate Docker volumes.\nThe solution can be easily tested locally and will behave the same locally as on a production server.\nSecuring the monitoring infrastructure To encrypt the traffic coming in and out of our monitoring infrastructure, we used Traefik which can automatically renew and obtain TLS certificates for our monitoring infrastructure from Let’s Encrypt. Traefik is an open source edge router that acts as a reverse proxy for our Docker containers. It can be deployed using official Docker images, so it integrates perfectly into our Docker Compose application.\nWe defined separate Traefik services and routers for Grafana and InfluxDB that take care of appropriate routing, HTTP/HTTPS redirection, and TLS configuration. This was all done in a declarative way using labels in our Docker compose file.\nGrafana supports user authentication, which can be integrated with many different services including Azure and GitHub. This also allows for easy management of permissions of the users accessing our Grafana dashboards.\nDeployment of Telegraf Telegraf can be installed on the machines running Corda in a variety of ways and works as a stand-alone tool. Setting it up is relatively straightforward, so you can choose whichever installation method most suits your setup.\nComplete infrastructure architecture The complete architecture of our infrastructure looks as follows:\nTLS is terminated at our reverse proxy (Traefik). This means that traffic between the proxy and Grafana/InfluxDB is not encrypted, but this isn’t an issue since all these services are running in a single secured machine.\nFinal result We set up a Grafana dashboard with metrics for each Corda node in our cluster. The dashboard features high-level flow metrics first, followed by internal operation metrics (P2P, caches), and finally system-level metrics (JVM, disk IO, network). A part of this dashboard is shown below.\nWe also have a dashboard with a summary of the results from our performance testing suite, which helps us inspect the results quickly and identify potential regressions. A part of it that shows throughput numbers for some of our test cases is shown below. This is made possible by sending data from our test suite running JMeter to InfluxDB using the JMeter InfluxDB Backend Listener.\nThe “Difference” column displays the results of a Flux query performed on our InfluxDB database, repeated for every test case using Grafana’s variable feature. It calculates the difference in results of each test case between the currently specified run and a run that happened a certain time ago (in this example 24 hours ago).\nConclusion A complete solution for monitoring Corda nodes can be set up entirely using open source tools without compromises. Many of the tools can be mixed and matched, so it’s possible to adjust the foundation described in this post to better fit your individual needs and existing setup.\nOne of the most impactful choices to be made when setting up a monitoring solution for Corda nodes is the choice of TSDB, as that will greatly affect the performance and usability of your dashboards. InfluxDB and Prometheus are strong TSDB options which have many discerning features that can make one more favourable over the other, depending on your requirements.\nFor more information on monitoring Corda nodes, check the following articles:\nMonitoring Corda Nodes With Prometheus, Grafana and ELK on Docker\nMonitoring Corda Nodes using Prometheus and Grafana\nMonitoring Corda Nodes (Part 1)\nThe Corda documentation is also an amazing resource:\nNode metrics\nNode monitoring and logging\n","wordCount":"2056","inLanguage":"en","image":"https://dominikrys.com/img/cover.png","datePublished":"2020-09-21T16:52:48+01:00","dateModified":"2020-09-21T16:52:48+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dominikrys.com/posts/monitoring-corda-nodes/"},"publisher":{"@type":"Organization","name":"Dominik Rys","logo":{"@type":"ImageObject","url":"https://dominikrys.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dominikrys.com/ accesskey=h title="Dominik Rys (Alt + H)">Dominik Rys</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dominikrys.com/tags title=🔖 Tags><span>🔖 Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf</h1><div class=post-meta><span title='2020-09-21 16:52:48 +0100 +0100'>September 21, 2020</span>&nbsp;·&nbsp;10 min</div></header><figure class=entry-cover><img loading=eager srcset="https://dominikrys.com/posts/monitoring-corda-nodes/img/cover_hu1410268468762850144.png 360w ,https://dominikrys.com/posts/monitoring-corda-nodes/img/cover_hu4606396475532221498.png 480w ,https://dominikrys.com/posts/monitoring-corda-nodes/img/cover_hu2629152820632692724.png 720w ,https://dominikrys.com/posts/monitoring-corda-nodes/img/cover_hu4051264975036742584.png 1080w ,https://dominikrys.com/posts/monitoring-corda-nodes/img/cover_hu11604440643780405535.png 1500w ,https://dominikrys.com/posts/monitoring-corda-nodes/img/cover.png 3509w" sizes="(min-width: 768px) 720px, 100vw" src=https://dominikrys.com/posts/monitoring-corda-nodes/img/cover.png alt width=3509 height=1788></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#intro aria-label=Intro>Intro</a></li><li><a href=#hosting-the-monitoring-infrastructure aria-label="Hosting the monitoring infrastructure">Hosting the monitoring infrastructure</a><ul><li><a href=#third-party-managed-services aria-label="Third-party managed services">Third-party managed services</a></li><li><a href=#self-hosting aria-label=Self-hosting>Self-hosting</a></li></ul></li><li><a href=#comparison-of-tools aria-label="Comparison of tools">Comparison of tools</a><ul><li><a href=#metric-collection-agent aria-label="Metric collection agent">Metric collection agent</a></li><li><a href=#time-series-database aria-label="Time-series database">Time-series database</a></li><li><a href=#front-end-for-visualising-and-querying-the-tsdb aria-label="Front end for visualising and querying the TSDB">Front end for visualising and querying the TSDB</a></li></ul></li><li><a href=#extra-considerations aria-label="Extra considerations">Extra considerations</a><ul><li><a href=#why-deploy-in-docker aria-label="Why deploy in Docker?">Why deploy in Docker?</a></li><li><a href=#securing-the-monitoring-infrastructure aria-label="Securing the monitoring infrastructure">Securing the monitoring infrastructure</a></li><li><a href=#deployment-of-telegraf aria-label="Deployment of Telegraf">Deployment of Telegraf</a></li></ul></li><li><a href=#complete-infrastructure-architecture aria-label="Complete infrastructure architecture">Complete infrastructure architecture</a></li><li><a href=#final-result aria-label="Final result">Final result</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><blockquote><p><strong>This post is also hosted on the <a href=https://www.corda.net/blog/monitoring-corda-nodes-using-grafana-influxdb-and-telegraf/>Corda Blog</a>.</strong> The main goal behind this post was to provide an easily accessible high-level overview on monitoring Corda nodes. It also showcases part of what I&rsquo;ve done during my summer internship at R3.</p></blockquote><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>Here at R3, we have a cluster of Corda nodes that we use for performance testing. We have developed a <a href=https://docs.corda.net/docs/corda-enterprise/performance-testing/toc-tree.html>performance testing suite</a> that enables us to establish baseline numbers, quantify improvements from new features, and identify regressions.</p><p>We recently decided to invest some effort in improving the observability of the overall system so that we could identify regressions and analyse their root cause more efficiently and with less manual work. There is <a href=https://docs.corda.net/docs/corda-enterprise/node-metrics.html>a wealth of metrics exposed by Corda nodes via JMX</a> that can be inspected using tools such as <a href=https://hawt.io/>Hawtio</a> (as <a href=https://docs.corda.net/docs/corda-enterprise/node/operating/monitoring-logging.html>described in the Corda docs</a>). However, this approach requires plenty of manual intervention and the prerequisite of the test actively running during inspection times.</p><figure class=align-center><img loading=lazy src=img/hawtio.png#center alt="Corda JMX metrics visible in Hawtio"><figcaption><p>Corda JMX metrics visible in Hawtio</p></figcaption></figure><p>We had to set up a monitoring infrastructure that would allow us to:</p><ul><li><p>Collect the metrics exposed by our Corda nodes via JMX.</p></li><li><p>Collect the metrics not exposed via JMX, such as disk IO and network activity.</p></li><li><p>Store the collected metrics in a centralised database.</p></li><li><p>Visualise, filter, and compare metrics from different time windows using a front end accessible from a web browser.</p></li></ul><p>Monitoring is a core aspect of operating Corda nodes efficiently. Therefore, in this post we give you a quick overview of the technologies available and their trade-offs. We also walk you through the capabilities and the architecture of our solution. The described work has been performed on Corda 4.5, but the high-level architecture is really version-agnostic. We hope this can help those of you getting started with monitoring!</p><h2 id=hosting-the-monitoring-infrastructure>Hosting the monitoring infrastructure<a hidden class=anchor aria-hidden=true href=#hosting-the-monitoring-infrastructure>#</a></h2><p>The first step was to decide how to host the monitoring infrastructure, as that would greatly impact the choice of other tools that we could use. Ultimately it came down to either using a third-party managed service or deploying the infrastructure ourselves in the public cloud.</p><h3 id=third-party-managed-services>Third-party managed services<a hidden class=anchor aria-hidden=true href=#third-party-managed-services>#</a></h3><p>This would be a great option if we wanted a scalable solution without us having to spend too much time setting up the infrastructure and managing it, or if it was to support production workloads that would have strict requirements for high availability. Given that we only wanted to monitor the nodes in our cluster and didn’t intend for it to scale beyond that, using a third-party service wouldn’t have been cost-effective as it would have provided many more features than were necessary for us.</p><p>We also wanted the ability to have multiple users access dashboards simultaneously, and the possibility for user authentication. These features were only accessible in the higher tiers of such services, which provide much more storage and bandwidth than we required.</p><h3 id=self-hosting>Self-hosting<a hidden class=anchor aria-hidden=true href=#self-hosting>#</a></h3><p>We ended up going down the path of hosting the monitoring infrastructure ourselves in Microsoft Azure. The most popular tools for setting up monitoring solutions provide open source offerings so this was a viable option. It also has the added benefits of giving us full control over the infrastructure and knowing the exact running costs.</p><h2 id=comparison-of-tools>Comparison of tools<a hidden class=anchor aria-hidden=true href=#comparison-of-tools>#</a></h2><p>As we chose to self-host the monitoring infrastructure, we had the liberty of choosing from the multitude of open source tools available to set up our infrastructure. We carefully considered which tools to use so that the monitoring infrastructure wouldn’t require much effort to maintain in the long run.</p><p>Effectively the monitoring infrastructure can be split up into three parts:</p><ol><li><p>A metric collection agent.</p></li><li><p>A time-series database (TSDB).</p></li><li><p>A front end for visualising and querying the TSDB.</p></li></ol><p>The descriptions of each part below provide details about the tool we chose to use, followed by alternatives that we had also considered.</p><h3 id=metric-collection-agent>Metric collection agent<a hidden class=anchor aria-hidden=true href=#metric-collection-agent>#</a></h3><figure class=align-center><img loading=lazy src=img/telegraf-logo.png#center alt="Telegraf Logo"></figure><p>⭐ <a href=https://www.influxdata.com/time-series-platform/telegraf><strong>Telegraf</strong></a>— a lightweight open source server agent that <a href=https://docs.influxdata.com/telegraf/latest/plugins/>can collect and write metrics to and from different sources</a>. It’s plug-in driven so we could easily set it up using the provided <a href=https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2>jolokia2 plug-in</a> to collect the metrics exposed through JMX from our Corda nodes.</p><p>Telegraf also provides plug-ins that allow for monitoring various system metrics which aren’t available through JMX, such as disk IO and networking usage. In our case, it was so convenient to configure that we also deployed it on our <a href=https://docs.corda.net/docs/corda-os/node-database.html>Corda node databases</a> with the relevant <a href=https://github.com/influxdata/telegraf/tree/master/plugins/inputs/postgresql>PostgreSQL</a> and <a href=https://github.com/influxdata/telegraf/tree/master/plugins/inputs/sqlserver>SQL Server</a> plug-ins.</p><p><strong>Other options</strong> — we also considered <a href=https://github.com/prometheus/jmx_exporter><strong>Prometheus JMX exporter</strong></a>, <a href=https://github.com/logzio/jmx2graphite><strong>jmx2graphite</strong></a>, and <a href=https://github.com/jmxtrans/jmxtrans><strong>jmxtrans</strong></a>. The issue with these is that they are all limited to which metrics they can record and where they can send them to. Telegraf can provide essentially the same functionality as those tools and allows for extensibility, while greatly reducing the number of tools required for maintenance.</p><h3 id=time-series-database>Time-series database<a hidden class=anchor aria-hidden=true href=#time-series-database>#</a></h3><figure class=align-center><img loading=lazy src=img/influxdb-logo.png#center alt="InfluxDB Logo"></figure><p>⭐ <a href=https://www.influxdata.com/products/influxdb-overview/><strong>InfluxDB</strong></a> — an open source TSDB from InfluxData, who also develop Telegraf. It’s <a href=https://docs.influxdata.com/influxdb/v1.8/introduction/install/>easy to install on many different platforms</a> and can be interacted with using the SQL-like <a href=https://docs.influxdata.com/influxdb/v1.8/query_language/>InfluxQL</a> query language.</p><p>This is the TSDB we ended up using. So far it’s been working well, but a slight gripe with it is that the default query language (InfluxQL) is not quite powerful enough for certain tasks, such as performing calculations on data over different time windows. This is remedied by using InfluxData’s new <a href=https://www.influxdata.com/products/flux>Flux</a> query language, albeit at the cost of convenience due to a lack of simplified GUI for it in TSDB front ends — the queries have to be written in plain text. Before choosing InfluxDB we’d recommend checking <a href=https://github.com/influxdata/influxdb/issues/5930>this aggregate GitHub issue</a> to see if you’d heavily rely on any query operators that have not yet been implemented in InfluxQL.</p><p>Overall, Flux is still significantly more powerful than Prometheus’ and Graphite’s query languages. InfluxDB can be the best option if you don’t mind sacrificing some ease of use sometimes for the ability to write (almost) any query imaginable.</p><figure class=align-center><img loading=lazy src=img/prometheus-logo.png#center alt="Prometheus Logo"></figure><p><a href=https://prometheus.io><strong>Prometheus</strong></a> — another popular open source TSDB, which is entirely community-driven. It uses a similar <a href=https://prometheus.io/docs/introduction/comparison/#summary-0>data compression algorithm to InfluxDB</a>. The query language (<a href=https://prometheus.io/docs/prometheus/latest/querying/basics>PromQL</a>) is more robust than InfluxQL so you can do more out of the box, although it doesn’t resemble any particular language so requires learning from scratch.</p><p>One of the biggest differences in Prometheus compared to other TSDBs is that it <a href=https://prometheus.io/docs/introduction/faq/#why-do-you-pull-rather-than-push>pulls instead of pushing metrics</a>. This has some advantages, but also requires additional setting-up on the machines that run your Corda nodes to allow Prometheus to pick up the exported metrics, which entails opening extra ports and setting up firewall rules.</p><p>It was tough choosing between Prometheus and InfluxDB, but ultimately we went with InfluxDB due to it being maintained by the same people as Telegraf, the potential to write complex queries using Flux and requiring less setup to collect metrics from our Corda nodes.</p><figure class=align-center><img loading=lazy src=img/graphite-logo.png#center alt="Graphite Logo"></figure><p><a href=https://graphiteapp.org><strong>Graphite</strong></a> — the grandaddy of modern TSDBs. Graphite has been around for longer than Prometheus and InfluxDB, so it’s a mature and tested tool. The query language resembles some programming languages so it’s easy to pick up, and it can do more than InfluxQL and PromQL thanks to the huge selection of functions that have been developed over the years.</p><p>Being the oldest of the bunch has its disadvantages though, most notably that the <a href=https://www.influxdata.com/blog/influxdb-outperforms-graphite-in-time-series-data-metrics-benchmark>performance is lacklustre compared to InfluxDB</a> which would have to be accounted for by using a more powerful host VM. Installing Graphite can also be a pain due to its <a href=https://graphite.readthedocs.io/en/latest/install.html#dependencies>many dependencies</a> if it’s not deployed in Docker (inspiring projects such as <a href=https://github.com/obfuscurity/synthesize/>Synthesize</a> that are meant to make the process easier).</p><h3 id=front-end-for-visualising-and-querying-the-tsdb>Front end for visualising and querying the TSDB<a hidden class=anchor aria-hidden=true href=#front-end-for-visualising-and-querying-the-tsdb>#</a></h3><figure class=align-center><img loading=lazy src=img/example-grafana-dashboard.jpg#center alt="Example Grafana Dashboard"><figcaption><p>Example Grafana Dashboard</p></figcaption></figure><p>⭐ <a href=https://grafana.com><strong>Grafana</strong></a> — an open source tool for interactively visualising data from various data sources. Grafana is by far the most popular tool for interacting with TSDBs with over <a href=https://github.com/grafana/grafana>1200 contributors on GitHub</a> and a very active <a href=https://community.grafana.com/>community forum</a>. There are <a href=https://grafana.com/grafana/plugins>many plug-ins available for it</a>, it integrates well with many other services for <a href=https://grafana.com/docs/grafana/latest/alerting/notifications/>alerting</a> and <a href=https://grafana.com/docs/grafana/latest/auth/overview/>authentication</a>, and it makes creating aesthetically pleasing dashboards a breeze.</p><figure class=align-center><img loading=lazy src=img/example-chronograf-dashboard.png#center alt="Example Chronograf Dashboard"><figcaption><p>Example Chronograf Dashboard</p></figcaption></figure><p><a href=https://www.influxdata.com/time-series-platform/chronograf><strong>Chronograf</strong></a> — an open source tool for interacting and visualising data from InfluxDB. Chronograf is very well suited for setups where other products from InfluxData are used, as it’s better integrated with them compared to Grafana. This could have been a great option if we also used <a href=https://www.influxdata.com/time-series-platform/kapacitor/>Kapacitor</a> as a real-time streaming data processing engine from InfluxData, to complete their “<a href=https://www.influxdata.com/blog/introduction-to-influxdatas-influxdb-and-tick-stack/>TICK</a>” stack.</p><p>Given that Chronograf has fewer features than Grafana and <a href=https://github.com/influxdata/chronograf/graphs/contributors>is significantly less popular</a>, which can make getting support for it more difficult, we went with Grafana.</p><h2 id=extra-considerations>Extra considerations<a hidden class=anchor aria-hidden=true href=#extra-considerations>#</a></h2><h3 id=why-deploy-in-docker>Why deploy in Docker?<a hidden class=anchor aria-hidden=true href=#why-deploy-in-docker>#</a></h3><figure class=align-center><img loading=lazy src=img/docker-logo.png#center alt="Docker Logo"></figure><p>As all tools we chose provided official Docker images, we decided to deploy our monitoring infrastructure as a <a href=https://docs.docker.com/compose>Docker Compose</a> application in an Azure VM. This has many benefits:</p><ul><li><p>Deploying the monitoring infrastructure is a one-step process and is easily reproducible.</p></li><li><p>Updating and downgrading individual services is straightforward — just adjust the version of the Docker images!</p></li><li><p>It’s easy to manage each container’s data as it’s kept in separate Docker volumes.</p></li><li><p>The solution can be easily tested locally and will behave the same locally as on a production server.</p></li></ul><h3 id=securing-the-monitoring-infrastructure>Securing the monitoring infrastructure<a hidden class=anchor aria-hidden=true href=#securing-the-monitoring-infrastructure>#</a></h3><figure class=align-center><img loading=lazy src=img/traefik-logo.png#center alt="Traefik Logo"></figure><p>To encrypt the traffic coming in and out of our monitoring infrastructure, we used <a href=https://docs.traefik.io/>Traefik</a> which can automatically renew and obtain TLS certificates for our monitoring infrastructure from <a href=https://letsencrypt.org>Let’s Encrypt</a>. Traefik is an open source edge router that acts as a reverse proxy for our Docker containers. It can be deployed using <a href=https://hub.docker.com/_/traefik>official Docker images</a>, so it integrates perfectly into our Docker Compose application.</p><p>We defined separate Traefik <a href=https://docs.traefik.io/routing/services/>services</a> and <a href=https://docs.traefik.io/routing/routers/>routers</a> for Grafana and InfluxDB that take care of appropriate routing, HTTP/HTTPS redirection, and TLS configuration. This was all done in a declarative way using <a href=https://docs.traefik.io/routing/providers/docker/>labels in our Docker compose file</a>.</p><p>Grafana supports <a href=https://grafana.com/docs/grafana/latest/auth/overview>user authentication</a>, which can be integrated with many different services including Azure and GitHub. This also allows for easy management of permissions of the users accessing our Grafana dashboards.</p><h3 id=deployment-of-telegraf>Deployment of Telegraf<a hidden class=anchor aria-hidden=true href=#deployment-of-telegraf>#</a></h3><p>Telegraf can be installed on the machines running Corda in a <a href=https://docs.influxdata.com/telegraf/latest/introduction/installation>variety of ways</a> and works as a stand-alone tool. Setting it up is relatively straightforward, so you can choose whichever installation method most suits your setup.</p><h2 id=complete-infrastructure-architecture>Complete infrastructure architecture<a hidden class=anchor aria-hidden=true href=#complete-infrastructure-architecture>#</a></h2><p>The complete architecture of our infrastructure looks as follows:</p><figure class=align-center><img loading=lazy src=img/architecture-diagram.png#center alt="Architecture Diagram"></figure><p>TLS is terminated at our reverse proxy (Traefik). This means that traffic between the proxy and Grafana/InfluxDB is not encrypted, but this isn’t an issue since all these services are running in a single secured machine.</p><h2 id=final-result>Final result<a hidden class=anchor aria-hidden=true href=#final-result>#</a></h2><p>We set up a Grafana dashboard with metrics for each Corda node in our cluster. The dashboard features high-level flow metrics first, followed by internal operation metrics (P2P, caches), and finally system-level metrics (JVM, disk IO, network). A part of this dashboard is shown below.</p><figure class=align-center><img loading=lazy src=img/dashboard-1.png#center alt="Dashboard 1"></figure><figure class=align-center><img loading=lazy src=img/dashboard-2.png#center alt="Dashboard 2"></figure><p>We also have a dashboard with a summary of the results from our performance testing suite, which helps us inspect the results quickly and identify potential regressions. A part of it that shows throughput numbers for some of our test cases is shown below. This is made possible by sending data from our test suite running JMeter to InfluxDB using the <a href=https://jmeter.apache.org/usermanual/realtime-results.html>JMeter InfluxDB Backend Listener</a>.</p><figure class=align-center><img loading=lazy src=img/jmeter-results-dashboard.png#center alt="JMeter Results Dashboard"></figure><p>The “Difference” column displays the results of a Flux query performed on our InfluxDB database, repeated for every test case using <a href=https://grafana.com/docs/grafana/latest/variables/repeat-panels-or-rows/>Grafana’s variable feature</a>. It calculates the difference in results of each test case between the currently specified run and a run that happened a certain time ago (in this example 24 hours ago).</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>A complete solution for monitoring Corda nodes can be set up entirely using open source tools without compromises. Many of the tools can be mixed and matched, so it’s possible to adjust the foundation described in this post to better fit your individual needs and existing setup.</p><p>One of the most impactful choices to be made when setting up a monitoring solution for Corda nodes is the choice of TSDB, as that will greatly affect the performance and usability of your dashboards. InfluxDB and Prometheus are strong TSDB options which have many discerning features that can make one more favourable over the other, depending on your requirements.</p><p>For more information on monitoring Corda nodes, check the following articles:</p><ul><li><p><a href=https://www.corda.net/blog/monitoring-corda-nodes-with-prometheus-grafana-and-elk-on-docker-2/>Monitoring Corda Nodes With Prometheus, Grafana and ELK on Docker</a></p></li><li><p><a href=https://www.corda.net/blog/monitoring-corda-nodes-using-prometheus-and-grafana/>Monitoring Corda Nodes using Prometheus and Grafana</a></p></li><li><p><a href=https://www.corda.net/blog/monitoring-corda-nodes-part-1/>Monitoring Corda Nodes (Part 1)</a></p></li></ul><p>The Corda documentation is also an amazing resource:</p><ul><li><p><a href=https://docs.corda.net/docs/corda-enterprise/node-metrics.html>Node metrics</a></p></li><li><p><a href=https://docs.corda.net/docs/corda-enterprise/node/operating/monitoring-logging.html>Node monitoring and logging</a></p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://dominikrys.com/tags/docker/>Docker</a></li><li><a href=https://dominikrys.com/tags/site-reliability-engineering/>Site Reliability Engineering</a></li><li><a href=https://dominikrys.com/tags/influxdb/>InfluxDB</a></li><li><a href=https://dominikrys.com/tags/grafana/>Grafana</a></li><li><a href=https://dominikrys.com/tags/traefik/>Traefik</a></li></ul><nav class=paginav><a class=prev href=https://dominikrys.com/posts/monitoring-influxdb-grafana-traefik/><span class=title>« Prev</span><br><span>Setting up a TLS-Secured Monitoring Solution in Docker using InfluxDB, Grafana, and Traefik</span>
</a><a class=next href=https://dominikrys.com/posts/compiling-chip8-to-wasm/><span class=title>Next »</span><br><span>Compiling My C++ CHIP-8 Emulator to WebAssembly</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf on x" href="https://x.com/intent/tweet/?text=Monitoring%20Corda%20Nodes%20Using%20Grafana%2c%20InfluxDB%2c%20and%20Telegraf&amp;url=https%3a%2f%2fdominikrys.com%2fposts%2fmonitoring-corda-nodes%2f&amp;hashtags=Docker%2cSiteReliabilityEngineering%2cInfluxDB%2cGrafana%2cTraefik"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdominikrys.com%2fposts%2fmonitoring-corda-nodes%2f&amp;title=Monitoring%20Corda%20Nodes%20Using%20Grafana%2c%20InfluxDB%2c%20and%20Telegraf&amp;summary=Monitoring%20Corda%20Nodes%20Using%20Grafana%2c%20InfluxDB%2c%20and%20Telegraf&amp;source=https%3a%2f%2fdominikrys.com%2fposts%2fmonitoring-corda-nodes%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdominikrys.com%2fposts%2fmonitoring-corda-nodes%2f&title=Monitoring%20Corda%20Nodes%20Using%20Grafana%2c%20InfluxDB%2c%20and%20Telegraf"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdominikrys.com%2fposts%2fmonitoring-corda-nodes%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf on whatsapp" href="https://api.whatsapp.com/send?text=Monitoring%20Corda%20Nodes%20Using%20Grafana%2c%20InfluxDB%2c%20and%20Telegraf%20-%20https%3a%2f%2fdominikrys.com%2fposts%2fmonitoring-corda-nodes%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf on telegram" href="https://telegram.me/share/url?text=Monitoring%20Corda%20Nodes%20Using%20Grafana%2c%20InfluxDB%2c%20and%20Telegraf&amp;url=https%3a%2f%2fdominikrys.com%2fposts%2fmonitoring-corda-nodes%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Monitoring Corda Nodes Using Grafana, InfluxDB, and Telegraf on ycombinator" href="https://news.ycombinator.com/submitlink?t=Monitoring%20Corda%20Nodes%20Using%20Grafana%2c%20InfluxDB%2c%20and%20Telegraf&u=https%3a%2f%2fdominikrys.com%2fposts%2fmonitoring-corda-nodes%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//dominikrys.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://dominikrys.com/>Dominik Rys</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>